{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter3_MN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIP/rT2F0a1OPzEyPW30zZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnuDihYcXQyU"
      },
      "source": [
        "### Undone exercise and problems\n",
        "  - Ongoing problems: 8 and 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-19svdZo82C"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NldtAzB2FIr"
      },
      "source": [
        "1. Verify that $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$\n",
        "  - $\\sigma(z)\\equiv\\frac{1}{1+e^{-z}}$\n",
        "  - $\\sigma'(z)=\\frac{\\partial}{\\partial x}(1+e^{-z})^{-1}=(1+e^{-z})^{-2}e^{-z}=\\frac{e^{-z}}{(1+e^{-z})^2}$\n",
        "  - Add 1 and subtract 1 on the numerator:\n",
        "    - $\\frac{1+e^{-z}-1}{(1+e^{-z})^2}=\\frac{1+e^{-z}}{(1+e^{-z})^2}-\\frac{1}{(1+e^{-z})^2}=\\frac{1}{(1+e^{-z})}-\\frac{1}{(1+e^{-z})^2}=\\sigma(z)-\\sigma(z)^2=\\sigma(z)(1-\\sigma(z))$\n",
        "  - Hence, $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9g75u5NivQt"
      },
      "source": [
        "2. One gotcha with the cross-entropy is that it can be diffuclt at first to remember the respective roles of the $y$s and $a$s. It's easy to get confused about whether the right form is $-[y\\ ln(a) + (1-y)ln(1-a)]$ or $-[a\\ ln(y) + (1-a)ln(1-y)]$. What happens to the second of these expressions when $y=0$ or $1$? Does this problem afflict the first expression? Why or why not?\n",
        "  - At $y=0$ and $y=1$, the second expression is $-[a\\ ln(0) + (1-a)ln(1)]$, and $-[a\\ ln(1) + (1-a)ln(0)]$ respectively. $ln(0)$ is undefined so the second expression cannot be the right form for the cross-entropy.\n",
        "  - $-[y\\ ln(a) + (1-y)ln(1-a)]$ is not affected by any value of $y$. However, there would be a chance of having the same problem when $a=0$ or $a=1$, because $ln(0)$ is not defined. However, such case won't occur because $a$ is a sigmoid function and its inputs must be either $-\\infty$ or $\\infty$ so as to be either $a=0$ or $a=1$. Practically, $-\\infty$ or $\\infty$ is unachievable. Thus, this problem will not afflict the first expression.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_-GdbKwEFvX"
      },
      "source": [
        "3. In the single neuron discussion at the start of this section, I argued that the cross-entropy is small if $\\sigma (z)\\approx y$ for all training inputs. The argument relied on $y$ being equal to either 0 or 1. This is usually true in classification problems, but for other problems (e.g., regression problems) $y$ can sometimes take values intermediate between 0 and 1. Show that the cross entropy is still minimized when $\\sigma (z)=y$ for all training inputs. When this is the case the cross-entropy has the value: $C= -\\frac {1}{n} \\sum_{x}[y\\ ln\\ y+(1-y)\\ ln\\ (1-y)]$\\\n",
        " The quantity $-[y\\ ln\\ y+(1-y)\\ ln\\ (1-y)]$ is sometimes known as the binary entropy.\n",
        "  - $C_x = -[y\\ ln\\ y+(1-y)\\ ln\\ (1-y)]$ when $\\sigma(z)=y$. This is exactly same as the binary entrophy and it can be shown as below:\n",
        "\n",
        "    <div>\n",
        "<img src = \"https://raw.githubusercontent.com/cheul0518/Courses/main/NeuralNetworksAndDeepLearning/imgs/binaryEntropy_v2.png\" width = \"30%\">\n",
        "</div>\n",
        "  - As you see in the graph, $C_x$ is minimized as $y$ approaches either $0$ or $1$ for a arbitrary input $x$. $C$ is an average of $C_x$, and so cross entropy is still minimized when $\\sigma (z)=y$ for all training inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5emfhJwEQcfK"
      },
      "source": [
        "4. Construct an example showing explicitly that in a network with a sigmoid output layer, the output activations $a^L_j$ won't always sum to 1.\n",
        "  - Suppose there is a network that consists of a single input neuron and three output neurons. Weights and biases  are $w_{11}=1, w_{21}=1, w_{31}=1, b_1=0, b_2=0$, and $b_3=0$. When $x=0$, the weighted inputs are $z_1=0, z_2=0, z_3=0$. And so the activations are $a_1=0.5, a_2=0.5, a_3=0.5$, which is $\\sum_j {a_j} = 1.5$. Thus, the output activations $a^L_j$ won't always sum to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2EITta2YZaA"
      },
      "source": [
        "5. **Monotonicity of softmax**. Show that $\\frac{\\partial a^L_j}{\\partial z^L_k}$ is positive if $j=k$ and negative if $j\\neq k$. As a consequence, increasing $z^L_j$ is guaranteed to increase the corresponding output activation, $a^L_j$, and will decrease all the other output activations. We already saw this empirically with the sliders, but this is a rigorous proof.\n",
        "  - $\\frac{\\partial a^L_j}{\\partial z^L_k} = \\frac{\\partial}{\\partial z^L_k}(\\frac{e^{z^L_j}}{\\sum_k (e^{z^L_k})})$\n",
        "  - when $j\\neq k$, \n",
        "  \\\n",
        "  $\\frac{\\partial a^L_j}{\\partial z^L_k} = \\frac{\\partial}{\\partial z^L_k}\\frac{e^{z^L_j}}{\\sum_k (e^{z^L_k})} = \\frac{\\partial}{\\partial z^L_k}e^{z^L_j}(\\sum_ke^{z^L_k})^{-1} = -e^{z^L_j}(\\sum_ke^{z^L_k})^{-2}e^{z^L_k}$\n",
        "  \\\n",
        "  Because $e^{z^L_j}$, $(\\sum_ke^{z^L_k})^{-2}$, and $e^{z^L_k}$ are all positive numbers, $\\frac{\\partial a^L_j}{\\partial z^L_k} < 0$.\n",
        "  - when $j = k$,\n",
        "  \\\n",
        "  $\\frac{\\partial a^L_k}{\\partial z^L_k} = \\frac{\\partial}{\\partial z^L_k}\\frac{e^{z^L_k}}{\\sum_k (e^{z^L_k})} = \\frac{\\partial}{\\partial z^L_k}e^{z^L_k}(\\sum_k e^{z^L_k})^{-1} = e^{z^L_k}(\\sum_k e^{z^L_k})^{-1} - e^{z^L_k}(\\sum_k e^{z^L_k})^{-2}e^{z^L_k} = e^{z^L_k}(\\sum_k e^{z^L_k})^{-1} - (e^{z^L_k})^2(\\sum_k e^{z^L_k})^{-2}$\n",
        "  \\\n",
        "  Because $0 \\leq (e^{z^L_k})^2(\\sum_k e^{z^L_k})^{-2} \\leq e^{z^L_k}(\\sum_k e^{z^L_k})^{-1} \\leq 1$, $\\frac{\\partial a^L_j}{\\partial z^L_k} > 0$.\n",
        "  \\\n",
        "  \\\n",
        "  Thus, $\\frac{\\partial a^L_j}{\\partial z^L_k}$ is positive if $j=k$ and negative if $j\\neq k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4yBvVsNfL2h"
      },
      "source": [
        "6. **Non-locality of softmax**. A nice thing about sigmoid layers is that the output $a^L_j$ is a function of the corresponding weighted input, $a^L_j = \\sigma(z^L_j)$. Explain why this is not the case for a softmax layer: any particular output activation $a^L_j$ depends on all the weighted inputs.\n",
        "  - softmax layer output is $a^L_j = \\frac {e^{z^L_j}}{\\sum_ke^{z^L_k}}$. That is, every weigted input affects the value of $a^L_j$. You cannot have an isolate, unaffected value of $a^L_j$ of a softmax layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfQmuks53IvZ"
      },
      "source": [
        "7. As discussed above, one way of expanding the MNIST training data is to use small rotations of training images. What's a problem that might occur if we allow arbitraritly large rotations of training images?\n",
        "  - Let's say we rotate a digit, \"6\", by 180 degree. If such largely rotated data's trained, then a digit,\"9\", is likely to be recognized as \"6\". Thus, the trained neural network would not properly classify 6 and 9, lowering its classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY1MHTKz1o1q"
      },
      "source": [
        "8. Verify that the standard deviation of $z = \\sum_jw_jx_j + b$ in the paragraph above is $\\sqrt{\\frac{3}{2}}$. It may help to know that: (a) the variance of a sum of independent random variables is the sum of the variances of the individual random variales; and (b) the variance is the square of the standard deviation: (a) $V(x+y) = V(x)+V()$, and (b) $Std(z) = \\sqrt{V(z)}$.\n",
        "  - $V(w) = \\frac{1}{1000}, V(b) = 1$.\n",
        "  - $V(z) = V(\\sum_jw_jx_j + b) = \\sum_jV(w_jx_j) + V(b) = 500\\times\\frac{1}{1000} + 1 = \\frac{3}{2}$\n",
        "  - $Std(z) = \\sqrt{V(z)} = \\sqrt{\\frac{3}{2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO23o_hxVQsb"
      },
      "source": [
        "9. Modify network2.py so that it implements a learning schedule that: halves the learning rate each time the validation accuracy satisfies the no-improvement-in-10 rule; and terminates when the learning rate has dropped to 1/128 of its original value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yJe0EccZycY"
      },
      "source": [
        "#@title network with learing rate schedule\n",
        "\"\"\"network2.py\n",
        "~~~~~~~~~~~~~~\n",
        "A modified version of network2.py. Improvements include early stopping and \n",
        "learning rate schedule etc.\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Define the quadratic and cross-entropy cost functions\n",
        "class QuadraticCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a, y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output \"y\".\"\"\"\n",
        "          return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.\"\"\"\n",
        "          return (a-y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a,y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output\n",
        "          \"y\".  Note that np.nan_to_num is used to ensure numerical stability. \n",
        "          In particular, if both \"a\" and \"y\" have a 1.0 in the same slot, then \n",
        "          the expression (1-y)*np.log(1-a) returns nan.  The np.nan_to_num \n",
        "          ensures that that is converted to the correct value (0.0).\"\"\"\n",
        "          return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.  Note that the\n",
        "          parameter \"z\" is not used by the method.  It is included in the \n",
        "          method's parameters in order to make the interface consistent with \n",
        "          the delta method for other cost classes.\"\"\"\n",
        "          return (a-y)\n",
        "\n",
        "#### Main Network class\n",
        "class Network_lrs(object):\n",
        "\n",
        "      def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "          \"\"\"The list \"sizes\" contains the number of neurons in the respective\n",
        "          layers of the network.  For example, if the list was [2, 3, 1]\n",
        "          then it would be a three-layer network, with the first layer \n",
        "          containing 2 neurons, the second layer 3 neurons, and the third layer \n",
        "          1 neuron.  The biases and weights for the network are initialized \n",
        "          randomly, using \"self.default_weight_initializer\".\"\"\"\n",
        "          self.num_layers = len(sizes)\n",
        "          self.sizes = sizes\n",
        "          self.default_weight_initializer()\n",
        "          self.cost = cost\n",
        "\n",
        "\n",
        "      def default_weight_initializer(self):\n",
        "          \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1 over the square root of the number of\n",
        "          weights connecting to the same neuron.  Initialize the biases\n",
        "          using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "          Note that the first layer is assumed to be an input layer, and\n",
        "          by convention we won't set any biases for those neurons, since\n",
        "          biases are only ever used in computing the outputs from later layers.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def large_weight_initializer(self):\n",
        "          \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1.  Initialize the biases using a Gaussian \n",
        "          distribution with mean 0 and standard deviation 1. Note that the first\n",
        "          layer is assumed to be an input layer, and by convention we won't set \n",
        "          any biases for those neurons, since biases are only ever used in \n",
        "          computing the outputs from later layers. This weight and bias \n",
        "          initializer uses the same approach as in Chapter 1, and is included \n",
        "          for purposes of comparison.  It will usually be better to use the \n",
        "          default weight initializer instead.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randdn(y, x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def feedforward(self, a):\n",
        "          \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "          for b,w in zip(self.biases, self.weights):\n",
        "              a = sigmoid(np.dot(w,a)+b)\n",
        "          return a\n",
        "\n",
        "\n",
        "      def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
        "              evaluation_data=None, early_stopping=-1, monitor_evaluation_cost=False,\n",
        "              monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
        "              monitor_training_accuracy=False):\n",
        "          \"\"\"Train the neural network using mini-batch stochastic gradient\n",
        "          descent.  The \"training_data\" is a list of tuples \"(x, y)\"\n",
        "          representing the training inputs and the desired outputs.  The\n",
        "          other non-optional parameters are self-explanatory, as is the \n",
        "          regularization parameter \"lmbda\".  The method also accepts \n",
        "          \"evaluation_data\", usually either the validation or test data.  We can\n",
        "          monitor the cost and accuracy on either the evaluation data or the \n",
        "          training data, by setting the appropriate flags. The method returns \n",
        "          a tuple containing four lists: the (per-epoch) costs on the evaluation \n",
        "          data, the accuracies on the evaluation data, the costs on the training\n",
        "          data, and the accuracies on the training data. All values are evaluated \n",
        "          at the end of each training epoch. So, for example, if we train \n",
        "          for 30 epochs, then the first element of the tuple will be a 30-element\n",
        "          list containing the cost on the evaluation data at the end of each epoch. \n",
        "          Note that the lists are empty if the corresponding flag is not set.\"\"\"\n",
        "          if evaluation_data:\n",
        "              evaluation_data = list(evaluation_data)\n",
        "              n_data = len(evaluation_data)\n",
        "          training_data = list(training_data)\n",
        "          n = len(training_data)\n",
        "          eta_init = eta\n",
        "\n",
        "          while eta > eta_init*(2**-7):\n",
        "                self.default_weight_initializer()\n",
        "                evaluation_cost, evaluation_accuracy = [], []\n",
        "                training_cost, training_accuracy = [], []            \n",
        "                early_stopping = early_stopping            \n",
        "                print(f\"\\n<Current learning rate is {eta}>\")\n",
        "                for j in range(epochs):\n",
        "                    random.shuffle(training_data)\n",
        "                    mini_batches = [training_data[k:k+mini_batch_size] \n",
        "                                    for k in range(0, n, mini_batch_size)]\n",
        "                    for mini_batch in mini_batches:\n",
        "                        self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "                    print(f\"\\tEpoch {j} training complete\")\n",
        "                    if monitor_training_cost:\n",
        "                        cost = self.total_cost(training_data, lmbda)\n",
        "                        training_cost.append(cost)\n",
        "                        print(f\"\\tCost on training data: {cost}\")\n",
        "                    if monitor_training_accuracy:\n",
        "                        accuracy = self.accuracy(training_data, convert=True)\n",
        "                        training_accuracy.append(accuracy)\n",
        "                        print(f\"\\tAccuracy on training data: {accuracy}/{n}\")\n",
        "                    if monitor_evaluation_cost:\n",
        "                        cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                        evaluation_cost.append(cost)\n",
        "                        print(f\"\\tCost on evaluation data: {cost}\")\n",
        "                    if monitor_evaluation_accuracy:\n",
        "                        accuracy = self.accuracy(evaluation_data)\n",
        "                        evaluation_accuracy.append(accuracy)\n",
        "                        print(f\"\\tAccuracy on evaluation data: {100*accuracy/n_data:.2f}%\")\n",
        "                    # Early stopping\n",
        "                    i_max = evaluation_accuracy.index(max(evaluation_accuracy))\n",
        "                    if len(evaluation_accuracy[i_max:-1]) == early_stopping:\n",
        "                        print(f\"\\t///Early stopped///\")\n",
        "                        print(f\"\\tHighest classification accuracy: {100*evaluation_accuracy[i_max]/n_data:.2f}% at Epoch {i_max}\")\n",
        "                        break\n",
        "                eta = eta/2\n",
        "          print(\"\\nEta is the same as Eta/128 so the process's automatically terminated.\")                \n",
        "          # return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "      def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "          \"\"\"Update the network's weights and biases by applying gradient\n",
        "          descent using backpropagation to a single mini batch.  The \"mini_batch\" \n",
        "          is a list of tuples \"(x, y)\", \"eta\" is the learning rate, \"lmbda\" is \n",
        "          the regularization parameter, and \"n\" is the total size of the training \n",
        "          data set.\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "          # Partition the given minibatch into two groups: X and Y\n",
        "          X = [mini_batch[i][0] for i in range(len(mini_batch))]\n",
        "          Y = [mini_batch[i][1] for i in range(len(mini_batch))]\n",
        "          delta_nabla_b, delta_nabla_w = self.backprop(X,Y)\n",
        "          nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
        "          nabla_w = [nw+dnw for nw,dnw in zip(nabla_w, delta_nabla_w)]\n",
        "          self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "          self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw \n",
        "                          for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "      def backprop(self, x, y):\n",
        "          \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for \n",
        "          the cost function C_x. \"nabla_b\" and \"nabla_w\" are layer-by-layer lists \n",
        "          of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "          # Make X and Y ndarrays from lists\n",
        "          X = np.concatenate(x, axis=1)\n",
        "          Y = np.concatenate(y, axis=1)\n",
        "          # Feedforward\n",
        "          activation = X\n",
        "          activations = [X] # list to store all the activations, layer by layer \n",
        "          zs = [] # list to store all the z vectors, layer by layer\n",
        "          for b, w in zip(self.biases, self.weights):\n",
        "              z = np.dot(w, activation) + b\n",
        "              zs.append(z)\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "          # Backward pass\n",
        "          delta = (self.cost).delta(zs[-1], activations[-1], Y)\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "          for l in range(2, self.num_layers):\n",
        "              z = zs[-l]\n",
        "              sp = sigmoid_prime(z)\n",
        "              delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "              nabla_b[-l] = delta\n",
        "              nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "          # each ndarry in nabla_b gotta be summed along columns, and reshaped\n",
        "          for i in range(len(nabla_b)):\n",
        "              nabla_b[i] = nabla_b[i].sum(axis=1).reshape(nabla_b[i].shape[0],1)\n",
        "          return nabla_b, nabla_w\n",
        "\n",
        "      def accuracy(self, data, convert=False):\n",
        "          \"\"\"Return the number of inputs in \"data\" for which the neural network \n",
        "          outputs the correct result. The neural network's output is assumed to \n",
        "          be the index of whichever neuron in the final layer has the highest \n",
        "          activation. The flag \"convert\" should be set to False if the data set \n",
        "          is validation or test data (the usual case), and to True if the data \n",
        "          set is the training data. The need for this flag arises due to \n",
        "          differences in the way the results \"y\" are represented in the different \n",
        "          data sets. In particular, it flags whether we need to convert between \n",
        "          the different representations.  It may seem strange to use different\n",
        "          representations for the different data sets.  Why not use the same \n",
        "          representation for all three data sets?  It's done for efficiency \n",
        "          reasons - the program usually evaluates the cost on the training data \n",
        "          and the accuracy on other data sets. These are different types of \n",
        "          computations, and using different representations speeds things up.\n",
        "          More details on the representations can be found in mnist_loader.load_data_wrapper.\"\"\"\n",
        "          if convert:\n",
        "              results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                          for (x,y) in data]\n",
        "          else:\n",
        "              results = [(np.argmax(self.feedforward(x)), y)\n",
        "                          for (x,y) in data]\n",
        "          return sum(int(x==y) for (x,y) in results)\n",
        "\n",
        "      def total_cost(self, data, lmbda, convert=True):\n",
        "          \"\"\"Return the total cost for the data set \"data\". The flag \"convert\" \n",
        "          should be set to False if the data set is the training data (the usual \n",
        "          case), and to True if the data set is the validation or test data.\n",
        "          See comments on the similar (but reversed) convention for the \"accuracy\" method, above.\"\"\"\n",
        "          cost = 0.0\n",
        "          for x, y in data:\n",
        "              a = self.feedforward(x)\n",
        "              if convert:\n",
        "                  y = vectorized_results(y)\n",
        "              cost += self.cost.fn(a,y)/len(data)\n",
        "          cost += 0.5(lmbda/len(data))*sum(np.linalg.norm(w)**2\n",
        "                                           for w in self.weights)\n",
        "          return cost\n",
        "\n",
        "      def save(self, filename):\n",
        "          \"\"\"Save the neural network to the file \"filename\".\"\"\"\n",
        "          data = {\"sizes\": self.sizes,\n",
        "                  \"weights\": [w.tolist() for w in self.weights],\n",
        "                  \"biases\": [b.tolist() for b in self.biases],\n",
        "                  \"cost\": str(self.cost.__name__)}\n",
        "          f = open(filename, \"w\")\n",
        "          json.dump(data, f)        \n",
        "          f.close()\n",
        "\n",
        "\n",
        "#### Loading a Network\n",
        "def load(filename):\n",
        "    \"\"\"Load a neural network from the file \"filename\". Returns an instance of Network.\"\"\"\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position and \n",
        "    zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding \n",
        "    desired output from the neural network.\"\"\"\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mScSpKhT7ps3"
      },
      "source": [
        "#@title Download the MNIST data set and minist_loader.py\n",
        "#Download the dataset from Nielsen's github\n",
        "!wget -L https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
        "\n",
        "#Download mnist_loader.py from Dobrzanski's github: his version's for python 3\n",
        "!wget -L https://raw.githubusercontent.com/MichalDanielDobrzanski/DeepLearningPython/master/mnist_loader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfnzrNO57r7H"
      },
      "source": [
        "#@title Learning rate schedule\n",
        "import mnist_loader\n",
        "\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "net_lrs = Network_lrs([784,30,10])\n",
        "net_lrs.SGD(training_data, 30, 10, eta = 10, lmbda = 3.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True, early_stopping=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw_yhfqrfLhE"
      },
      "source": [
        "10. It's tempting to use gradient descent to try to learn good values for hyper-parameters such as $\\lambda$ and $\\eta$. Can you think of an obstacle to using gradient descent to determine $\\lambda$? Can you think of an obstacle to using gradient descent to determine $\\eta$?\n",
        "  - $C = C_0 + \\frac{\\lambda}{2n}L_k$, where $L_1 = \\sum_ww$ and $L_2 = \\sum_ww^2$\n",
        "  - The cost function above is $C(w,\\lambda)$. You can't compute the gradient of $\\eta$ with cost function that is nothing to do with the variable $\\eta$.\n",
        "  - From cost function, you can derive $\\frac{\\partial C}{\\partial \\lambda} = \\frac{L_k}{2n}$. This is a constant so that $\\lambda$ continuously decrease or increase. Thus, you cannot compute its extremum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLDgraQzNkUY"
      },
      "source": [
        "11. What would go wrong if we used $\\mu > 1$ in the momentum technique? What would go wrong if we used $\\mu < 0$ in the momentum technique?\n",
        "  - For $\\mu > 1$, the GD is likely to overshoot while for $\\mu < 0 $ it is likely to either be ridiculously slow($-1<\\mu< 0$) or overshoot($\\mu<-1$). Check the codes below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq033QoE0dtn"
      },
      "source": [
        "#@title 11. momentum comparison\n",
        "# Original codes from Jason Brownlee: https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/\n",
        "# The codes are a bit modifed to compare the momentum coefficient values\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"f(x) = x**2\"\"\"\n",
        "    return x**2\n",
        "def f_prime(x):\n",
        "    \"\"\"f'(x) = 2x\"\"\"\n",
        "    return 2*x\n",
        "\n",
        "def MGD(epoches, eta, mu):\n",
        "    \"\"\"It performs momentum gradient descent.\"\"\"\n",
        "    w_axis, C_axis = [],[]\n",
        "    w = 1.0\n",
        "    v = 0.0\n",
        "    for epoch in range(epoches):\n",
        "        nabla_C = f_prime(w)\n",
        "        v_update = mu * v - eta * nabla_C\n",
        "        w = w + v_update\n",
        "        v = v_update\n",
        "        C = f(w)\n",
        "        w_axis.append(w)\n",
        "        C_axis.append(C)\n",
        "    return w_axis, C_axis\n",
        "\n",
        "# Set x and y for comparison\n",
        "x = np.arange(-1.0, 1.1, 0.1)\n",
        "y = f(x)\n",
        "\n",
        "#fig1\n",
        "fig1 = pyplot.figure()\n",
        "fig1.suptitle(\"Momentum: 0.5, iteration: 15\")\n",
        "w1, C1 = MGD(epoches=15, eta=0.1, mu=0.5)\n",
        "pyplot.plot(x, y)\n",
        "pyplot.plot(w1,C1,'.-', color='red')\n",
        "\n",
        "#fig2\n",
        "fig2 = pyplot.figure()\n",
        "fig2.suptitle(\"Momentum: 2.0, iteration: 3\")\n",
        "w2, C2 = MGD(epoches=3, eta=0.1, mu=2.0)\n",
        "pyplot.plot(x, y)\n",
        "pyplot.plot(w2, C2,'.-', color='green')\n",
        "\n",
        "#fig3\n",
        "fig3 = pyplot.figure()\n",
        "fig3.suptitle(\"Momentum: -0.5, iteration: 15\")\n",
        "w3, C3 = MGD(epoches=15, eta=0.1, mu=-0.5)\n",
        "pyplot.plot(x, y)\n",
        "pyplot.plot(w3,C3,'.-', color='blue')\n",
        "\n",
        "#fig4\n",
        "fig4 = pyplot.figure()\n",
        "fig4.suptitle(\"Momentum: -3.0, iteration: 4\")\n",
        "w4, C4 = MGD(epoches=4, eta=0.1, mu=-3.0)\n",
        "pyplot.plot(x, y)\n",
        "pyplot.plot(w4,C4,'.-', color='pink')\n",
        "\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZiK_hAfmy6O"
      },
      "source": [
        "12. Prove the identity in equation $\\sigma(z) = \\frac{1+tanh(z/2)}{2}$\n",
        "  - $tanh(z) \\equiv \\frac{e^z-e^{-z}}{e^z+e^{-z}} = \\frac{1-e^{-2z}}{1+e^{-2z}}$. Subsititue $z$ for $\\frac{z}{2}$. Then $tanh(\\frac{z}{2})=\\frac{1-e^{-z}}{1+e^{-z}} = \\frac{1}{1+e^{-z}} - \\frac{e^{-z}}{1+e^{-z}}$. \n",
        "  - $\\sigma(z) = \\frac{1}{1+e^{-z}} \\to e^{-z} = \\frac{1-\\sigma(z)}{\\sigma(z)}$\n",
        "  - Subsitute $e^{-z}$ in $tanh(\\frac{z}{2})$, then $tanh(\\frac{z}{2}) = \\sigma(z) - (1-\\sigma(z)) = 2\\sigma(z) - 1$. \n",
        "  - Hence, $\\sigma(z) = \\frac{1 + tanh(z)}{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGbEVy-IMN-H"
      },
      "source": [
        "# Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU1NMl7cNFzA"
      },
      "source": [
        "1. **Many-layer multi-neuron networks**. In the notation introduced in the last chapter, show that for the quadratic cost the partial derivative with respect to weights in the output layer is $\\frac{\\partial C} {\\partial W^L_{jk}} = \\frac{1}{n}\\sum_x a^{L-1}_k (a^L_j - y_j)\\sigma'(z^L_j)$. The term $\\sigma'(z^L_j)$ causes a learning slowdown whenever an output neuron saturates on the wrong value. Show that for the cross-entropy cost the output error $\\delta^L$ for a single training example $x$ given by $\\delta = a^L - y$. Use this expression to show that the partial derivative with respect to the weights in the output layer is given by $\\frac{\\partial C} {\\partial W^L_{jk}} = \\frac{1}{n}\\sum_x a^{L-1}_k (a^L_j - y_j)$. The $\\sigma'(z^L_j)$ term has vanished , and so the cross entropy avoids the problem of learning slowdonw, not just when used with a single neuron, as we saw earlir, but also in many-layer multi-neuron networks. A simple variation on this analysis holds also for the biases. If this is not obvious to you, then you should work through that analysis as well.\n",
        "  - For cross entropy cost, the output error $\\delta^L$ for a single training example $x$ is, $\\delta^L = \\frac{\\partial C_x}{\\partial z^L} = \\frac{\\partial C_x}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^L} = \\frac{\\partial C_x}{\\partial a^L}\\sigma'(z^L) = (\\frac{y}{a^L} + \\frac{1-y}{1-a^L})\\sigma'(z^L) = \\frac{y-a^L}{a^L(1-a)^L}\\sigma'(z^L) = \\frac{y-a^L}{a^L(1-a)^L}(1-a^L)a^L = y-a^L$\n",
        "  - For a single training example, $\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k \\delta^L_j = a^{L-1}_k (a^L_j - y_j)$. \n",
        "  - For all training examples, sum each $\\frac{\\partial C}{\\partial w^L_{jk}}$ per $x$ and divide by $n$, and so $\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n}\\sum_xa^{L-1}_k\\delta^L_j = \\frac{1}{n}\\sum_xa^{L-1}_k (a^L_j - y_j)$\n",
        "  - $\\frac{\\partial C}{\\partial b^L_j} = \\delta^L_j = y-a^L$ for a single training example.\n",
        "  \\\n",
        "  \\\n",
        "  Thus, both $\\frac{\\partial C}{\\partial w^L_{jk}}$ and $\\frac{\\partial C}{\\partial b^L_j}$ do not have $\\sigma'(z^L_j)$ that causes a learning slowdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5yUlBmD8xm1"
      },
      "source": [
        "2. **Using the quadratic cost when we have linear neurons in the output layer**. Suppose that we have a many-layer multi-neuron network. Suppose all the neurons in the final layer are linear neurons, meaning that the sigmoid activation function is not applied, and the outputs are simply $a^L_j = z^L_j$. Show that if we use the quadratic cost function then the output error $\\delta^L$ for a single training exdample $x$ is given by $\\delta^L = a^L - y$. Similarly to the previous problem, use this expression to show that the partial derivatives with respect to the weights and biases in the output layer are given by \n",
        "\\\n",
        "$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n}\\sum_x a^{L-1}_k(a^L_j-y_j)$\n",
        "\\\n",
        "$\\frac{\\partial C}{\\partial b^L_j} = \\frac{1}{n}\\sum_x (a^L_j-y_j)$. \n",
        "\\\n",
        " This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, infact, an appropriate cost function to use.\n",
        "  - $\\delta^L = \\frac {\\partial C}{\\partial a^L}\\sigma'(z^L)$, and $\\sigma'(z^L)$ is 1. So, $\\delta^L = \\frac {\\partial C}{\\partial a^L} = a^L_j-y_j$\n",
        "  -  $\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k \\delta^L_j = a^{L-1}_k(a^L_j-y_j)$. For all training examples of $x$, $\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n}\\sum_x a^{L-1}_k(a^L_j-y_j)$\n",
        "  - $\\frac{\\partial C}{\\partial b^L_j} = \\delta^L_j = a^L_j-y_j$. For all training examples of $x$, $\\frac{\\partial C}{\\partial b^L_j} = \\frac{1}{n}\\sum_xa^L_j-y_j$.\n",
        "  \\\n",
        "  \\\n",
        "Thus, the quaractic cost will not cause a learning slowdown if all the neurons in the final layers are linear neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7J0sqfP4Tcu"
      },
      "source": [
        "3. We've discussed at length the learning slowdown that can occur when output neurons saturate, in networks using the quadratic cost to train. Another factor that may inhibit learning is the presence of the $x_j$ term in equation $\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n}\\sum_xx_j(\\sigma(z)-y)$. Because of this term, when an input $x_j$ is near to zero, the corresponding weight $w_j$ will learn slowly. Explain why it is not possible to eliminate the $x_j$ term through a clevel choice of cost function.\n",
        "  - Let's say $\\frac{\\partial C}{\\partial w_j} = \\sigma(z)-y = a-y$ without $x_j$\n",
        "  - $\\frac{\\partial C}{\\partial w_j} = \\frac{\\partial C}{\\partial a}\\frac{\\partial a}{\\partial w_j} = \\frac{\\partial C}{\\partial a}x_j\\sigma'(z) = \\frac{\\partial C}{\\partial a}x_j(1-a)a = a-y$\n",
        "  - $\\frac{\\partial C}{\\partial a} = \\frac{a-y}{x_j(1-a)a}$, and $x_j$ occurs so that $x_j$ cannot be elimintaed from Cross Entropy.\n",
        "  - In general case, $\\frac{\\partial C}{\\partial a} = \\frac{math-expression}{x_j(1-a)a}$. In order to remove $x_j$, $\\frac{\\partial C}{\\partial w_j}$ is ought to be ($x_j\\times$(math expression)).  But we would like to have $\\frac{\\partial C}{\\partial w_j}$ without $x_j$ so it cannot be achievable.\n",
        "  \\\n",
        "  \\\n",
        "  Thus, elimintating the $x_j$ term is not possible in any clevel choice of cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEwDXHw5iBmZ"
      },
      "source": [
        "4. **Inverting the softmax layer**. Suppose we have a neural network with a softmax output layer, and the activations $a^L_j$ are known. Show that the corresponding weighted inputs have the form $z^L_j = ln\\ a^L_j+C$ for some constant $C$ that is indepdent of $j$\n",
        "  - $a^L_j = \\frac {e^{z^L_j}}{\\sum_ke^{z^L_k}} \\to a^L_j\\sum_ke^{z^L_k}=e^{z^L_j}$. Apply $ln$ to both sides, then $ln\\ a^L_j + ln\\sum_ke^{z^L_k}=ln\\ e^{z^L_j}$. $ln\\sum_ke^{z^L_k}$ is some constant $C$ that is independent of $j$. \n",
        "  \\\n",
        "  \\\n",
        "  Thus, $z^L_j=ln\\ a^L_j + C$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJeKSb9i3VQs"
      },
      "source": [
        "5. **Derive equations** $\\frac{\\partial C}{\\partial b^L_j} = a^L_j - y_j$ and $\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k(a^L_j - y_j)$\n",
        "  - $C \\equiv -ln\\ a^L_y, \\frac{\\partial C}{\\partial a^L_y} = -\\frac{1}{a^L_y}$\n",
        "  - $\\frac{\\partial C}{\\partial b^L_j} = \\sum_k \\frac{\\partial C}{\\partial a^L_k}\\frac{\\partial a^L_k}{\\partial b^L_j} = \\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial b^L_j}$, because $C$ is affected only by $a^L_y$. Now we have to consider two cases: $y=j$ or $y\\neq j$ because $\\frac{\\partial a^L_y}{\\partial b^L_j}$ would be different for each case.\n",
        "    - When $y=j$, $\\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial b^L_j} = -\\frac{1}{a^L_j}\\frac{\\partial}{\\partial b^L_j}exp(z^L_j)(\\sum_kexp(z^L_k))^{-1} = -\\frac{1}{a^L_j}(exp(z^L_j)(\\sum_kexp(z^L_k))^{-1}-exp(z^L_j)^2(\\sum_kexp(z^L_k))^{-2})=-\\frac{1}{a^L_j}(1-a^L_j)a^L_j = a^L_j - 1 = a^L_j - y_j$ \n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $j^{th}$ location. Thus, $1$ for here can be substituted for $y_j$.\n",
        "    - When $y\\neq j$, $\\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial b^L_j} = -\\frac{1}{a^L_y}\\frac{\\partial}{\\partial b^L_j}exp(z^L_y)(\\sum_kexp(z^L_k))^{-1} = \\frac{1}{a^L_y}exp(z^L_y)(\\sum_kexp(z^L_k))^{-2}exp(z^L_j) = \\frac{1}{a^L_y}a^L_ya^L_j = a^L_j = a^L_j - 0 = a^L_j - y_j$\n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $y^{th}$ location. Thus, $0$ for here can be substituted for $y_j$.\n",
        "      \n",
        "    - Thus,  $\\frac{\\partial C}{\\partial b^L_j} = a^L_j - y_j$\n",
        "\n",
        "  - $\\frac{\\partial C}{\\partial w^L_{jk}} = \\sum_i\\frac{\\partial C}{\\partial a^L_i}\\frac{\\partial a^L_i}{\\partial w^L_{jk}} = \\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial w^L_{jk}}$, because $C$ is affected only by $a^L_y$. Now we have to consider two cases: $y=j$ or $y\\neq j$ because $\\frac{\\partial a^L_y}{\\partial w^L_{jk}}$ would be different for each case.\n",
        "    - When $y=j$, $\\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial w^L_{jk}} = -\\frac{1}{a^L_j}\\frac{\\partial}{\\partial w^L_{jk}}exp(z^L_j)(\\sum_kexp(z^L_k))^{-1} = -\\frac{1}{a^L_j}(exp(z^L_j)(\\sum_kexp(z^L_k))^{-1}a^{L-1}_k - (exp(z^L_j)^2(\\sum_kexp(z^L_k))^{-2}a^{L-1}_k) = -\\frac{1}{a^L_j}(1-a^L_j)a^L_ja^{L-1}_k = a^{L-1}_k(a^L_j-1) = a^{L-1}_k(a^L_j-y_j)$\n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $j^{th}$ location. Thus, $1$ for here can be substituted for $y_j$.\n",
        "    - when $y\\neq j$, $\\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial w^L_{jk}} = -\\frac{1}{a^L_y}\\frac{\\partial}{\\partial w^L_{jk}}exp(z^L_y)(\\sum_kexp(z^L_k))^{-1} = \\frac{1}{a^L_y}exp(z^L_y)(\\sum_kexp(z^L_k))^{-2}a^{L-1}_kexp(z^L_j) = \\frac{1}{a^L_y}a^L_ya^{L-1}_Ka^L_j = a^{L-1}_ka^L_j = a^{L-1}_k(a^L_j-0) = a^{L-1}_k(a^L_j-y_j)$\n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $y^{th}$ location. Thus, $0$ for here can be substituted for $y_j$.\n",
        "\n",
        "    - Thus, $\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k(a^L_j - y_j)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMC-ehcPFA-_"
      },
      "source": [
        "6. **Where does the \"softmax\" name come from?** Suppose we change the softmax function so the output activations are given by $a^L_j = \\frac{e^{cz^L_j}}{\\sum_ke^{cz^L_k}}$, where $c$ is a positive constant. Note that $c=1$ corresponds to the standard softmax function. But if we use a different value of $c$ we get a different function, which is nonethelss qualitatively rather similar to the softmax. In particular, show that the output activations form a probability distribution, just as for the usual softmax. Suppose we allow $c$ to become large, i.e., $c \\to \\infty$. What is the limiting value for the output activations $a^L_j$? After solving this problem it should be clear to you why we think of the $c=1$ function as a \"softened\" version of the maximum function. This is the original of the term \"softmax\".\n",
        "  - The output activations satisfies all the kolmogorov axioms: $P(X\\in E)\\geq 0\\ \\forall E \\in A$,where $E$ is some event, and $A$ is event space; $P(A) = 1$; $P(X \\in \\sqcup E_i) = \\sum_i P(X \\in E_i)$. Thus, the output activations form a probability distribution.\n",
        "  - The limiting value for the output activations $a^L_j$: $\\lim_{c\\to \\infty} a^L_j = \\lim_{c \\to \\infty}\\frac{e^{cz^L_j}}{\\sum_ke^{cz^L_k}}\\ \\approx 1$ if $z^L_j$ is the maximum weighted input; otherwise, $\\lim_{c\\to \\infty} a^L_j = \\lim_{c \\to \\infty}\\frac{e^{cz^L_j}}{\\sum_ke^{cz^L_k}}\\ \\approx 0$. This is because exponential function increases the maximum input a lot more rapidly than other non maximum inputs. For $n$ multiple maximum weighted inputs: $\\lim_{c\\to \\infty} a^L_j = \\lim_{c \\to \\infty}\\frac{e^{cz^L_j}}{\\sum_ke^{cz^L_k}}\\ \\approx \\frac{1}{n}$. \n",
        "  - Thus, this function becomes a smooth approximation to the maximum function(technically, the arg max function because it depends on the index of $j$), as $c \\to \\infty$. \n",
        "  - We see for $c > 1$, $a^L_j$ works as the hardmax. What about $c < 1$? Let's say $c = \\frac{1}{100}$. If there is a difference of 5, then $a^L_j(0,5) = (\\frac{1}{1+e^\\frac{1}{20}},\\frac{e^\\frac{1}{20}}{1+e^\\frac{1}{20}}) = (0.51,0.49)$. Now the value is not close to the argmax. For $c=1$, $a^L_j(0,5) = (\\frac{1}{1+e^5},\\frac{e^5}{1+e^5}) =(0.007,0.993)$, which is a softened version of the hardmax (0,1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfMgnvWatXFM"
      },
      "source": [
        "7. **Backpropagation with softmax and thelog-likelihood cost**. In the last chapter we derived the backpropagation algorithm for a network containing sigmoid layers. To apply the algorithm to a network with a softmax layer we need to figure out an expression for the error $\\delta^L_j \\equiv \\frac{\\partial C}{\\partial z^L_j}$ in the final layer. Show that a suitable expression is $\\delta^L_j = a^L_j - y_j$. Using this expression we can apply the backpropagation algortihm to a network using a softmax output layer and the log-likelihood cost.\n",
        "  - $\\delta^L_j \\equiv \\frac{\\partial C}{\\partial z^L_j} = \\sum_k\\frac{\\partial C}{\\partial a^L_k}\\frac{\\partial a^L_k}{\\partial z^L_j} =\\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial z^L_j}$ because $C$ is affected only by $a^L_y$. Now we have to consider two cases: $y=j$ or $y\\neq j$ because $\\frac{\\partial a^L_y}{\\partial z^L_j}$ would be different for each case.\n",
        "    - When $y=j$, $\\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j} = -\\frac{1}{a^L_j}\\frac{\\partial}{\\partial z^L_j}exp(z^L_j)(\\sum_kexp(z^L_k))^{-1} = -\\frac{1}{a^L_j}(exp(z^L_j)(\\sum_kexp(z^L_k))^{-1}-exp(z^L_j)^2(\\sum_kexp(z^L_k))^{-2})=-\\frac{1}{a^L_j}(1-a^L_j)a^L_j = a^L_j - 1 = a^L_j - y_j$\n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $j^{th}$ location. Thus, $1$ for here can be substituted for $y_j$.\n",
        "    - When $y\\neq j$, $\\frac{\\partial C}{\\partial a^L_y}\\frac{\\partial a^L_y}{\\partial z^L_j} = -\\frac{1}{a^L_y}\\frac{\\partial}{\\partial z^L_j}exp(z^L_y)(\\sum_kexp(z^L_k))^{-1} = \\frac{1}{a^L_y}exp(z^L_y)(\\sum_kexp(z^L_k))^{-2}exp(z^L_j) = \\frac{1}{a^L_y}a^L_ya^L_j = a^L_j = a^L_j - 0 = a^L_j - y_j$\n",
        "      - $y_j$ is a vector whose all 0s is except for a 1 in the $y^{th}$ location. Thus, $0$ for here can be substituted for $y_j$.\n",
        "  - Thus, $\\delta^L_j = a^L_j - y_j$\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVy80Ona_k7o"
      },
      "source": [
        "8. **(Research problem)** How do our machine learning algorithms perform in the limit of very large data sets? For any given algorithm it's natural to attempt to define a notion of asymptotic performance in the limit of truly big data. A quick-and-dirty approach to this problem is to simply try fitting curves to graphs like those shown above, and then to extrapolate the fitted curves out to infinity. An objection to this approach is that different approaches to curve fitting will give different notions of asymptotic performance. Can you find a principled justification for fitting to some particular class of curves? If so, compare the asymptotic performance of several different machine learning algorithms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDf5zz-7pNpi"
      },
      "source": [
        "#@title SGD with L2\n",
        "\"\"\"network2.py\n",
        "~~~~~~~~~~~~~~\n",
        "A modified version of network2.py. Improvements include L2 regularization and\n",
        "matrix multiplication. For caution, this code does not include unzip in its code\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Define the quadratic and cross-entropy cost functions\n",
        "class QuadraticCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a, y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output \"y\".\"\"\"\n",
        "          return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.\"\"\"\n",
        "          return (a-y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a,y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output\n",
        "          \"y\".  Note that np.nan_to_num is used to ensure numerical stability. \n",
        "          In particular, if both \"a\" and \"y\" have a 1.0 in the same slot, then \n",
        "          the expression (1-y)*np.log(1-a) returns nan.  The np.nan_to_num \n",
        "          ensures that that is converted to the correct value (0.0).\"\"\"\n",
        "          return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.  Note that the\n",
        "          parameter \"z\" is not used by the method.  It is included in the \n",
        "          method's parameters in order to make the interface consistent with \n",
        "          the delta method for other cost classes.\"\"\"\n",
        "          return (a-y)\n",
        "\n",
        "#### Main Network class\n",
        "class Network_L2(object):\n",
        "\n",
        "      def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "          \"\"\"The list \"sizes\" contains the number of neurons in the respective\n",
        "          layers of the network.  For example, if the list was [2, 3, 1]\n",
        "          then it would be a three-layer network, with the first layer \n",
        "          containing 2 neurons, the second layer 3 neurons, and the third layer \n",
        "          1 neuron.  The biases and weights for the network are initialized \n",
        "          randomly, using \"self.default_weight_initializer\".\"\"\"\n",
        "          self.num_layers = len(sizes)\n",
        "          self.sizes = sizes\n",
        "          self.default_weight_initializer()\n",
        "          self.cost = cost\n",
        "\n",
        "\n",
        "      def default_weight_initializer(self):\n",
        "          \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1 over the square root of the number of\n",
        "          weights connecting to the same neuron.  Initialize the biases\n",
        "          using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "          Note that the first layer is assumed to be an input layer, and\n",
        "          by convention we won't set any biases for those neurons, since\n",
        "          biases are only ever used in computing the outputs from later layers.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def large_weight_initializer(self):\n",
        "          \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1.  Initialize the biases using a Gaussian \n",
        "          distribution with mean 0 and standard deviation 1. Note that the first\n",
        "          layer is assumed to be an input layer, and by convention we won't set \n",
        "          any biases for those neurons, since biases are only ever used in \n",
        "          computing the outputs from later layers. This weight and bias \n",
        "          initializer uses the same approach as in Chapter 1, and is included \n",
        "          for purposes of comparison.  It will usually be better to use the \n",
        "          default weight initializer instead.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randdn(y, x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def feedforward(self, a):\n",
        "          \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "          for b,w in zip(self.biases, self.weights):\n",
        "              a = sigmoid(np.dot(w,a)+b)\n",
        "          return a\n",
        "\n",
        "\n",
        "      def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
        "              evaluation_data=None, monitor_evaluation_cost=False,\n",
        "              monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
        "              monitor_training_accuracy=False):\n",
        "          \"\"\"Train the neural network using mini-batch stochastic gradient\n",
        "          descent.  The \"training_data\" is a list of tuples \"(x, y)\"\n",
        "          representing the training inputs and the desired outputs.  The\n",
        "          other non-optional parameters are self-explanatory, as is the \n",
        "          regularization parameter \"lmbda\".  The method also accepts \n",
        "          \"evaluation_data\", usually either the validation or test data.  We can\n",
        "          monitor the cost and accuracy on either the evaluation data or the \n",
        "          training data, by setting the appropriate flags. The method returns \n",
        "          a tuple containing four lists: the (per-epoch) costs on the evaluation \n",
        "          data, the accuracies on the evaluation data, the costs on the training\n",
        "          data, and the accuracies on the training data. All values are evaluated \n",
        "          at the end of each training epoch. So, for example, if we train \n",
        "          for 30 epochs, then the first element of the tuple will be a 30-element\n",
        "          list containing the cost on the evaluation data at the end of each epoch. \n",
        "          Note that the lists are empty if the corresponding flag is not set.\"\"\"\n",
        "          if evaluation_data:\n",
        "              n_data = len(evaluation_data)\n",
        "          n = len(training_data)\n",
        "          evaluation_cost, evaluation_accuracy = [], []\n",
        "          training_cost, training_accuracy = [], []\n",
        "\n",
        "          for j in range(epochs):\n",
        "              random.shuffle(training_data)\n",
        "              mini_batches = [training_data[k:k+mini_batch_size] \n",
        "                              for k in range(0, n, mini_batch_size)]\n",
        "              for mini_batch in mini_batches:\n",
        "                  self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "#              print(f\"Epoch {j} training complete\")\n",
        "              if monitor_training_cost:\n",
        "                  cost = self.total_cost(training_data, lmbda)\n",
        "                  training_cost.append(cost)\n",
        "                  print(f\"Cost on training data: {cost}\")\n",
        "              if monitor_training_accuracy:\n",
        "                  accuracy = self.accuracy(training_data, convert=True)\n",
        "                  training_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on training data: {accuracy}/{n}\")\n",
        "              if monitor_evaluation_cost:\n",
        "                  cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                  evaluation_cost.append(cost)\n",
        "                  print(f\"Cost on evaluation data: {cost}\")\n",
        "              if monitor_evaluation_accuracy:\n",
        "                  accuracy = self.accuracy(evaluation_data)\n",
        "                  evaluation_accuracy.append(accuracy)\n",
        "                  #print(f\"Accuracy on evaluation data: {100*accuracy/n_data:.2f}%\")\n",
        "#          return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "          return evaluation_accuracy[-1]/100\n",
        "\n",
        "\n",
        "      def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "          \"\"\"Update the network's weights and biases by applying gradient\n",
        "          descent using backpropagation to a single mini batch.  The \"mini_batch\" \n",
        "          is a list of tuples \"(x, y)\", \"eta\" is the learning rate, \"lmbda\" is \n",
        "          the regularization parameter, and \"n\" is the total size of the training \n",
        "          data set.\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "          # Partition the given minibatch into two groups: X and Y\n",
        "          X = [mini_batch[i][0] for i in range(len(mini_batch))]\n",
        "          Y = [mini_batch[i][1] for i in range(len(mini_batch))]\n",
        "          delta_nabla_b, delta_nabla_w = self.backprop(X,Y)\n",
        "          nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
        "          nabla_w = [nw+dnw for nw,dnw in zip(nabla_w, delta_nabla_w)]\n",
        "          self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "          self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw \n",
        "                          for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "      def backprop(self, x, y):\n",
        "          \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for \n",
        "          the cost function C_x. \"nabla_b\" and \"nabla_w\" are layer-by-layer lists \n",
        "          of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "          # Make X and Y ndarrays from lists\n",
        "          X = np.concatenate(x, axis=1)\n",
        "          Y = np.concatenate(y, axis=1)\n",
        "          # Feedforward\n",
        "          activation = X\n",
        "          activations = [X] # list to store all the activations, layer by layer \n",
        "          zs = [] # list to store all the z vectors, layer by layer\n",
        "          for b, w in zip(self.biases, self.weights):\n",
        "              z = np.dot(w, activation) + b\n",
        "              zs.append(z)\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "          # Backward pass\n",
        "          delta = (self.cost).delta(zs[-1], activations[-1], Y)\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "          for l in range(2, self.num_layers):\n",
        "              z = zs[-l]\n",
        "              sp = sigmoid_prime(z)\n",
        "              delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "              nabla_b[-l] = delta\n",
        "              nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "          # each ndarry in nabla_b gotta be summed along columns, and reshaped\n",
        "          for i in range(len(nabla_b)):\n",
        "              nabla_b[i] = nabla_b[i].sum(axis=1).reshape(nabla_b[i].shape[0],1)\n",
        "          return nabla_b, nabla_w\n",
        "\n",
        "      def accuracy(self, data, convert=False):\n",
        "          \"\"\"Return the number of inputs in \"data\" for which the neural network \n",
        "          outputs the correct result. The neural network's output is assumed to \n",
        "          be the index of whichever neuron in the final layer has the highest \n",
        "          activation. The flag \"convert\" should be set to False if the data set \n",
        "          is validation or test data (the usual case), and to True if the data \n",
        "          set is the training data. The need for this flag arises due to \n",
        "          differences in the way the results \"y\" are represented in the different \n",
        "          data sets. In particular, it flags whether we need to convert between \n",
        "          the different representations.  It may seem strange to use different\n",
        "          representations for the different data sets.  Why not use the same \n",
        "          representation for all three data sets?  It's done for efficiency \n",
        "          reasons - the program usually evaluates the cost on the training data \n",
        "          and the accuracy on other data sets. These are different types of \n",
        "          computations, and using different representations speeds things up.\n",
        "          More details on the representations can be found in mnist_loader.load_data_wrapper.\"\"\"\n",
        "          if convert:\n",
        "              results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                          for (x,y) in data]\n",
        "          else:\n",
        "              results = [(np.argmax(self.feedforward(x)), y)\n",
        "                          for (x,y) in data]\n",
        "          return sum(int(x==y) for (x,y) in results)\n",
        "\n",
        "      def total_cost(self, data, lmbda, convert=True):\n",
        "          \"\"\"Return the total cost for the data set \"data\". The flag \"convert\" \n",
        "          should be set to False if the data set is the training data (the usual \n",
        "          case), and to True if the data set is the validation or test data.\n",
        "          See comments on the similar (but reversed) convention for the \"accuracy\" method, above.\"\"\"\n",
        "          cost = 0.0\n",
        "          for x, y in data:\n",
        "              a = self.feedforward(x)\n",
        "              if convert:\n",
        "                  y = vectorized_results(y)\n",
        "              cost += self.cost.fn(a,y)/len(data)\n",
        "          cost += 0.5(lmbda/len(data))*sum(np.linalg.norm(w)**2\n",
        "                                           for w in self.weights)\n",
        "          return cost\n",
        "\n",
        "      def save(self, filename):\n",
        "          \"\"\"Save the neural network to the file \"filename\".\"\"\"\n",
        "          data = {\"sizes\": self.sizes,\n",
        "                  \"weights\": [w.tolist() for w in self.weights],\n",
        "                  \"biases\": [b.tolist() for b in self.biases],\n",
        "                  \"cost\": str(self.cost.__name__)}\n",
        "          f = open(filename, \"w\")\n",
        "          json.dump(data, f)        \n",
        "          f.close()\n",
        "\n",
        "\n",
        "#### Loading a Network\n",
        "def load(filename):\n",
        "    \"\"\"Load a neural network from the file \"filename\". Returns an instance of Network.\"\"\"\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position and \n",
        "    zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding \n",
        "    desired output from the neural network.\"\"\"\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOUzUNPP1YW2"
      },
      "source": [
        "#@title Download the MNIST data set and minist_loader.py\n",
        "#Download the dataset from Nielsen's github\n",
        "!wget -L https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
        "\n",
        "#Download mnist_loader.py from Dobrzanski's github: his version's for python 3\n",
        "!wget -L https://raw.githubusercontent.com/MichalDanielDobrzanski/DeepLearningPython/master/mnist_loader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_b0DJA1aWk"
      },
      "source": [
        "import mnist_loader\n",
        "\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "training_data = list(training_data)\n",
        "validation_data = list(validation_data)\n",
        "\n",
        "X, Y = [],[]\n",
        "\n",
        "for i in range(10):\n",
        "    net_L2 = Network_L2([784, 30, 10])  \n",
        "    X.append((len(training_data)/10)*(i+1))\n",
        "    Y.append(net_L2.SGD(training_data[:int((len(training_data)/10)*(i+1))], 30, 10, 0.5, lmbda=0.5, evaluation_data=validation_data, monitor_evaluation_accuracy=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMOV3tGrcCGE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from math import sin, cos\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def f_linear(X, a, b):\n",
        "    return [a*x + b for x in X]\n",
        "\n",
        "def f_2ndPoly(X, a, b, c):\n",
        "    return [a*x**2 + b*x + c for x in X]\n",
        "\n",
        "def f_3rdPoly(X, a, b, c, d):\n",
        "    return [a*x**3 + b*x**2 + c*x + d for x in X]\n",
        "\n",
        "def f_4thPoly(X, a, b, c, d, e):\n",
        "    return [a*x**4 + b*x**3 + c*x**2 + d*x +e for x in X]            \n",
        "\n",
        "# linear\n",
        "# Use non-linear least squares to find optimal values for the parameters \n",
        "# so that the sum of the squared residuals of f(xdata, *popt) - ydata is minimized.\n",
        "popt, _ = curve_fit(f_linear, X, Y)  # _ is used to ignore the value of pcov\n",
        "a, b = popt\n",
        "f_l = f_linear(X, a, b)\n",
        "\n",
        "print(f\"Linear f(x) = {a}x + {b:.2f}\")\n",
        "fig_l = plt.figure()\n",
        "fig_l.suptitle(\"SGD, linear function\")\n",
        "plt.scatter(X,Y, color='black')\n",
        "plt.plot(X,f_l,color='red')\n",
        "\n",
        "# 2nd polynomial\n",
        "popt, _ = curve_fit(f_2ndPoly, X, Y)  # _ is used to ignore the value of pcov\n",
        "a, b, c = popt\n",
        "f_2p = f_2ndPoly(X, a, b, c)\n",
        "\n",
        "print(f\"2nd polynomial f(x) = {a}x^2 + {b}x + {c:.2f}\")\n",
        "fig_2p = plt.figure()\n",
        "fig_2p.suptitle(\"SGD, polynomial function\")\n",
        "plt.scatter(X,Y, color='black')\n",
        "plt.plot(X,f_2p,'-', color='green')\n",
        "\n",
        "# 3rd polynomial\n",
        "popt, _ = curve_fit(f_3rdPoly, X, Y)  # _ is used to ignore the value of pcov\n",
        "a, b, c, d = popt\n",
        "f_3p = f_3rdPoly(X, a, b, c, d)\n",
        "\n",
        "print(f\"3rd polynomial f(x) = {a}x^3 + {b}x^2 + {c}x + {d:.2f}\")\n",
        "fig_3p = plt.figure()\n",
        "fig_3p.suptitle(\"SGD, polynomial function\")\n",
        "plt.scatter(X,Y, color='black')\n",
        "plt.plot(X,f_3p,'-', color='purple')\n",
        "\n",
        "# # 4th polynomial\n",
        "# popt, _ = curve_fit(f_4thPoly, X, Y)  # _ is used to ignore the value of pcov\n",
        "# a, b, c, d, e = popt\n",
        "# f_4p = f_4thPoly(X, a, b, c, d, e)\n",
        "\n",
        "# print(f\"4th polynomial f(x) = {a}x^4 + {b}x^3 + {c}x^2 + {d}x + {e:.2f}\")\n",
        "# fig_4p = plt.figure()\n",
        "# fig_4p.suptitle(\"SGD, polynomial function\")\n",
        "# plt.scatter(X,Y, color='black')\n",
        "# plt.plot(X,f_4p,'-', color='orange')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOntz5xDVGjc"
      },
      "source": [
        "9. **Connecting regularization and the imporved method of weight initalization** L2 regularizatoin sometimes automatically gives us something similar to the new approach to weight initialization. Suppose we are using the old approach to weight initialization. Sketch a heuristic argument that: (1) supposing $\\lambda$ is not too small, the first epochs of training will be dominated almost entirely by weight decay.\n",
        "  - We have a high variance of weighted inputs because we're using the old approach to weight initialization. The high variance of the weighted inputs in the network causes $\\sigma(z)$ in hidden layers to be very close to either 0 or 1, so that hidden neurons saturate. For the first epoch, in other words, $\\frac{\\partial C}{\\partial w}$ is minuscule while $w$ is relatively huge. Thus, the impact of weight decay on $w$ is dominating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kisA0UzGpbA"
      },
      "source": [
        "9. (2) Provided $\\eta\\lambda << n$ the weights will decay a factor of exp($-\\frac{\\eta\\lambda}{m}$) per epoch.\n",
        "  - First iteration of $w \\to (1-\\frac{\\eta\\lambda}{n})w - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}$. \n",
        "  - Second iteration of $w \\to (1-\\frac{\\eta\\lambda}{n})[(1-\\frac{\\eta\\lambda}{n})w - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}] - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} = (1-\\frac{\\eta\\lambda}{n})^2w - (1-\\frac{\\eta\\lambda}{n})\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}$\n",
        "  - Third iteration of $w \\to (1-\\frac{\\eta\\lambda}{n})^2[(1-\\frac{\\eta\\lambda}{n})w - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}] - (1-\\frac{\\eta\\lambda}{n})\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} = (1-\\frac{\\eta\\lambda}{n})^3w - (1-\\frac{\\eta\\lambda}{n})^2\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - (1-\\frac{\\eta\\lambda}{n})\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}$\n",
        "  - $k^{th}$ iteration of $w\\to(1-\\frac{\\eta\\lambda}{n})^kw - (1-\\frac{\\eta\\lambda}{n})^{k-1}\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\cdots - (1-\\frac{\\eta\\lambda}{n})\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}$ \n",
        "  - Because $k=\\frac{n}{m}$ per epoch, $w \\to (1-\\frac{\\eta\\lambda}{n})^{\\frac{n}{m}}w - (1-\\frac{\\eta\\lambda}{n})^{{\\frac{n}{m}}-1}\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\cdots - (1-\\frac{\\eta\\lambda}{n})\\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w} - \\frac{\\eta}{m}\\sum_x\\frac{\\partial C_x}{\\partial w}$. \n",
        "  - Accordingly, weight decay($1-\\frac{\\eta\\lambda}{n}$) occurs $\\frac{n}{m}$ times   as $n$ approaches $\\infty$. In other words, $\\lim_{n\\to\\infty }(1-\\frac{\\eta\\lambda}{n})^{\\frac{n}{m}}$. \n",
        "  - Suppose $y=\\lim_{n\\to\\infty }(1-\\frac{\\eta\\lambda}{n})^{\\frac{n}{m}}$. Take the log of both sides then $ln\\ y=\\lim_{n\\to\\infty }ln(1-\\frac{\\eta\\lambda}{n})^{\\frac{n}{m}}=\\lim_{n\\to\\infty }{\\frac{n}{m}}ln(1-\\frac{\\eta\\lambda}{n})=\\lim_{n\\to\\infty }\\frac{ln(1-\\frac{\\eta\\lambda}{n})}{\\frac{m}{n}}$. \n",
        "  - This form is $\\frac{0}{0}$, and it can be solved by L'H$\\hat{o}$pital's Rule: $lim_{x \\to a}\\frac{f(x)}{g(x)} = lim_{x \\to a}\\frac{f'(x)}{g'(x)} \\to lim_{n\\to\\infty}\\frac{\\partial ln(1-\\frac{\\eta\\lambda}{n})}{\\partial\\frac{m}{n}} = lim_{n\\to\\infty}\\frac{\\eta\\lambda}{n^2(1-\\frac{\\eta\\lambda}{n})}\\frac{-n^2}{m} = -\\frac{\\eta\\lambda}{m}$, and $ln\\ y = -\\frac{\\eta\\lambda}{m}$. \n",
        "  - Therefore, $y = exp(-\\frac{\\eta\\lambda}{m})$. The weights decay a factor of $exp(-\\frac{\\eta\\lambda}{m})$ per epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkKMeMrUdwl"
      },
      "source": [
        "9. (3) supposing $\\lambda$ is not too large, the weight decay will tail off when the weights are down to a size around $\\frac{1}{\\sqrt{n}}$ where $n$ is the total number of weights in the network. Argue that these conditions are all satisfied in the examples graphed in this section.\n",
        " - Given $w$ and $b$ initialized with normalized Gaussian, the weighted inputs $z$ has a standard deviation of $\\sqrt{n_w\\times var(w)+n_b\\times var(b)}$, where $n_w$ and $n_b$ are the number of weigts and biases respectively. $var(b)$ is negligible so $z$ has a standard deviation of $\\sqrt{n_w\\times var(w)}$. For here, $z$ has a very broad Gaussian distribution so that $\\sigma(z)$ is likely to saturate, when $var(w)=1$ . In a regularized network, however, regularization plays a role of finding small weights. $w$ is distributed around standard deviation of $\\frac{1}{\\sqrt{n_w}}$ so that $z$ will have a standard deviation of $1$, which is 95% of $z$ is between -2 and 2. In other words, 95% of $\\sigma(z)$ has far from either 0 or 1, not saturating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm6vvtp2LFC3"
      },
      "source": [
        "10. Modify the code above to implement L1 regularization, and use L1 regularization to classify MNIST digits using a 30 hidden neuron network. Can you find a regularization parameter that enables you to do better than running unregularized?\n",
        "  - Given a list of $\\lambda$: $[0, \\frac{1}{2^2}, \\frac{1}{2}, 1, 2, 2^2, 2^3]$, the regularized networks with $\\lambda \\leq 1$ outperformed the unregularized network with $\\lambda: 0$. However, the result constantly changes every test so there is no absolute $\\lambda$ that helps the network outperform compared to the unregularized network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM31RFK5cK79"
      },
      "source": [
        "#@title Network with L1 and matrix multiplication\n",
        "\"\"\"network2.py\n",
        "~~~~~~~~~~~~~~\n",
        "A modified version of network2.py. Improvements include L1 regularization and\n",
        "matrix multiplication.\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Define the quadratic and cross-entropy cost functions\n",
        "class QuadraticCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a, y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output \"y\".\"\"\"\n",
        "          return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.\"\"\"\n",
        "          return (a-y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a,y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output\n",
        "          \"y\".  Note that np.nan_to_num is used to ensure numerical stability. \n",
        "          In particular, if both \"a\" and \"y\" have a 1.0 in the same slot, then \n",
        "          the expression (1-y)*np.log(1-a) returns nan.  The np.nan_to_num \n",
        "          ensures that that is converted to the correct value (0.0).\"\"\"\n",
        "          return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.  Note that the\n",
        "          parameter \"z\" is not used by the method.  It is included in the \n",
        "          method's parameters in order to make the interface consistent with \n",
        "          the delta method for other cost classes.\"\"\"\n",
        "          return (a-y)\n",
        "\n",
        "#### Main Network class\n",
        "class Network(object):\n",
        "\n",
        "      def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "          \"\"\"The list \"sizes\" contains the number of neurons in the respective\n",
        "          layers of the network.  For example, if the list was [2, 3, 1]\n",
        "          then it would be a three-layer network, with the first layer \n",
        "          containing 2 neurons, the second layer 3 neurons, and the third layer \n",
        "          1 neuron.  The biases and weights for the network are initialized \n",
        "          randomly, using \"self.default_weight_initializer\".\"\"\"\n",
        "          self.num_layers = len(sizes)\n",
        "          self.sizes = sizes\n",
        "          self.default_weight_initializer()\n",
        "          self.cost = cost\n",
        "\n",
        "\n",
        "      def default_weight_initializer(self):\n",
        "          \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1 over the square root of the number of\n",
        "          weights connecting to the same neuron.  Initialize the biases\n",
        "          using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "          Note that the first layer is assumed to be an input layer, and\n",
        "          by convention we won't set any biases for those neurons, since\n",
        "          biases are only ever used in computing the outputs from later layers.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def large_weight_initializer(self):\n",
        "          \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1.  Initialize the biases using a Gaussian \n",
        "          distribution with mean 0 and standard deviation 1. Note that the first\n",
        "          layer is assumed to be an input layer, and by convention we won't set \n",
        "          any biases for those neurons, since biases are only ever used in \n",
        "          computing the outputs from later layers. This weight and bias \n",
        "          initializer uses the same approach as in Chapter 1, and is included \n",
        "          for purposes of comparison.  It will usually be better to use the \n",
        "          default weight initializer instead.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randdn(y, x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def feedforward(self, a):\n",
        "          \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "          for b,w in zip(self.biases, self.weights):\n",
        "              a = sigmoid(np.dot(w,a)+b)\n",
        "          return a\n",
        "\n",
        "\n",
        "      def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
        "              evaluation_data=None, monitor_evaluation_cost=False,\n",
        "              monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
        "              monitor_training_accuracy=False):\n",
        "          \"\"\"Train the neural network using mini-batch stochastic gradient\n",
        "          descent.  The \"training_data\" is a list of tuples \"(x, y)\"\n",
        "          representing the training inputs and the desired outputs.  The\n",
        "          other non-optional parameters are self-explanatory, as is the \n",
        "          regularization parameter \"lmbda\".  The method also accepts \n",
        "          \"evaluation_data\", usually either the validation or test data.  We can\n",
        "          monitor the cost and accuracy on either the evaluation data or the \n",
        "          training data, by setting the appropriate flags. The method returns \n",
        "          a tuple containing four lists: the (per-epoch) costs on the evaluation \n",
        "          data, the accuracies on the evaluation data, the costs on the training\n",
        "          data, and the accuracies on the training data. All values are evaluated \n",
        "          at the end of each training epoch. So, for example, if we train \n",
        "          for 30 epochs, then the first element of the tuple will be a 30-element\n",
        "          list containing the cost on the evaluation data at the end of each epoch. \n",
        "          Note that the lists are empty if the corresponding flag is not set.\"\"\"\n",
        "          if evaluation_data:\n",
        "              evaluation_data = list(evaluation_data)\n",
        "              n_data = len(evaluation_data)\n",
        "          training_data = list(training_data)\n",
        "          n = len(training_data)\n",
        "          evaluation_cost, evaluation_accuracy = [], []\n",
        "          training_cost, training_accuracy = [], []\n",
        "\n",
        "          for j in range(epochs):\n",
        "              random.shuffle(training_data)\n",
        "              mini_batches = [training_data[k:k+mini_batch_size] \n",
        "                              for k in range(0, n, mini_batch_size)]\n",
        "              for mini_batch in mini_batches:\n",
        "                  self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "#              print(f\"Epoch {j} training complete\")\n",
        "              if monitor_training_cost:\n",
        "                  cost = self.total_cost(training_data, lmbda)\n",
        "                  training_cost.append(cost)\n",
        "                  print(f\"Cost on training data: {cost}\")\n",
        "              if monitor_training_accuracy:\n",
        "                  accuracy = self.accuracy(training_data, convert=True)\n",
        "                  training_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on training data: {accuracy}/{n}\")\n",
        "              if monitor_evaluation_cost:\n",
        "                  cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                  evaluation_cost.append(cost)\n",
        "                  print(f\"Cost on evaluation data: {cost}\")\n",
        "              if monitor_evaluation_accuracy:\n",
        "                  accuracy = self.accuracy(evaluation_data)\n",
        "                  evaluation_accuracy.append(accuracy)\n",
        "                  if j == (epochs-1):\n",
        "                      print(f\"Accuracy on evaluation data: {100*accuracy/n_data:.2f}%\")\n",
        "#          return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "      def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "          \"\"\"Update the network's weights and biases by applying gradient\n",
        "          descent using backpropagation to a single mini batch.  The \"mini_batch\" \n",
        "          is a list of tuples \"(x, y)\", \"eta\" is the learning rate, \"lmbda\" is \n",
        "          the regularization parameter, and \"n\" is the total size of the training \n",
        "          data set.\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "          # Partition the given minibatch into two groups: X and Y\n",
        "          X = [mini_batch[i][0] for i in range(len(mini_batch))]\n",
        "          Y = [mini_batch[i][1] for i in range(len(mini_batch))]\n",
        "          delta_nabla_b, delta_nabla_w = self.backprop(X,Y)\n",
        "          nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
        "          nabla_w = [nw+dnw for nw,dnw in zip(nabla_w, delta_nabla_w)]\n",
        "          self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "          self.weights = [w-eta*(lmbda/n)*np.sign(w)-(eta/len(mini_batch))*nw \n",
        "                          for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "      def backprop(self, x, y):\n",
        "          \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for \n",
        "          the cost function C_x. \"nabla_b\" and \"nabla_w\" are layer-by-layer lists \n",
        "          of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "          # Make X and Y ndarrays from lists\n",
        "          X = np.concatenate(x, axis=1)\n",
        "          Y = np.concatenate(y, axis=1)\n",
        "          # Feedforward\n",
        "          activation = X\n",
        "          activations = [X] # list to store all the activations, layer by layer \n",
        "          zs = [] # list to store all the z vectors, layer by layer\n",
        "          for b, w in zip(self.biases, self.weights):\n",
        "              z = np.dot(w, activation) + b\n",
        "              zs.append(z)\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "          # Backward pass\n",
        "          delta = (self.cost).delta(zs[-1], activations[-1], Y)\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "          for l in range(2, self.num_layers):\n",
        "              z = zs[-l]\n",
        "              sp = sigmoid_prime(z)\n",
        "              delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "              nabla_b[-l] = delta\n",
        "              nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "          # each ndarry in nabla_b gotta be summed along columns, and reshaped\n",
        "          for i in range(len(nabla_b)):\n",
        "              nabla_b[i] = nabla_b[i].sum(axis=1).reshape(nabla_b[i].shape[0],1)\n",
        "          return nabla_b, nabla_w\n",
        "\n",
        "      def accuracy(self, data, convert=False):\n",
        "          \"\"\"Return the number of inputs in \"data\" for which the neural network \n",
        "          outputs the correct result. The neural network's output is assumed to \n",
        "          be the index of whichever neuron in the final layer has the highest \n",
        "          activation. The flag \"convert\" should be set to False if the data set \n",
        "          is validation or test data (the usual case), and to True if the data \n",
        "          set is the training data. The need for this flag arises due to \n",
        "          differences in the way the results \"y\" are represented in the different \n",
        "          data sets. In particular, it flags whether we need to convert between \n",
        "          the different representations.  It may seem strange to use different\n",
        "          representations for the different data sets.  Why not use the same \n",
        "          representation for all three data sets?  It's done for efficiency \n",
        "          reasons - the program usually evaluates the cost on the training data \n",
        "          and the accuracy on other data sets. These are different types of \n",
        "          computations, and using different representations speeds things up.\n",
        "          More details on the representations can be found in mnist_loader.load_data_wrapper.\"\"\"\n",
        "          if convert:\n",
        "              results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                          for (x,y) in data]\n",
        "          else:\n",
        "              results = [(np.argmax(self.feedforward(x)), y)\n",
        "                          for (x,y) in data]\n",
        "          return sum(int(x==y) for (x,y) in results)\n",
        "\n",
        "      def total_cost(self, data, lmbda, convert=True):\n",
        "          \"\"\"Return the total cost for the data set \"data\". The flag \"convert\" \n",
        "          should be set to False if the data set is the training data (the usual \n",
        "          case), and to True if the data set is the validation or test data.\n",
        "          See comments on the similar (but reversed) convention for the \"accuracy\" method, above.\"\"\"\n",
        "          cost = 0.0\n",
        "          for x, y in data:\n",
        "              a = self.feedforward(x)\n",
        "              if convert:\n",
        "                  y = vectorized_results(y)\n",
        "              cost += self.cost.fn(a,y)/len(data)\n",
        "          cost += 0.5(lmbda/len(data))*sum(np.linalg.norm(w)**2\n",
        "                                           for w in self.weights)\n",
        "          return cost\n",
        "\n",
        "      def save(self, filename):\n",
        "          \"\"\"Save the neural network to the file \"filename\".\"\"\"\n",
        "          data = {\"sizes\": self.sizes,\n",
        "                  \"weights\": [w.tolist() for w in self.weights],\n",
        "                  \"biases\": [b.tolist() for b in self.biases],\n",
        "                  \"cost\": str(self.cost.__name__)}\n",
        "          f = open(filename, \"w\")\n",
        "          json.dump(data, f)        \n",
        "          f.close()\n",
        "\n",
        "\n",
        "#### Loading a Network\n",
        "def load(filename):\n",
        "    \"\"\"Load a neural network from the file \"filename\". Returns an instance of Network.\"\"\"\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position and \n",
        "    zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding \n",
        "    desired output from the neural network.\"\"\"\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYV_Lu-SuYcq"
      },
      "source": [
        "#@title Download the MNIST data set and minist_loader.py\n",
        "#Download the dataset from Nielsen's github\n",
        "!wget -L https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
        "\n",
        "#Download mnist_loader.py from Dobrzanski's github: his version's for python 3\n",
        "!wget -L https://raw.githubusercontent.com/MichalDanielDobrzanski/DeepLearningPython/master/mnist_loader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwchPjVWucSR",
        "outputId": "8031b4b8-c5ec-42a7-8d41-612f5389132b"
      },
      "source": [
        "#@title unregularized network vs regularized network\n",
        "import mnist_loader\n",
        "\n",
        "# Create a list of lambda\n",
        "lmbda = [0]\n",
        "for i in range(6):\n",
        "    lmbda.append(2**(i-2))\n",
        "\n",
        "# Compare accuracy on validation data, according to lambda\n",
        "for lmb in lmbda:\n",
        "    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "    net = Network([784, 30, 10])\n",
        "    if lmb == 0:\n",
        "        print(f\"Lambda is {lmb}: unregularized\")\n",
        "    else:\n",
        "        print(f\"Lambda is {lmb}: regularized\")        \n",
        "    net.SGD(training_data, 30, 10, 0.5, lmbda = lmb, evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lambda is 0: unregularized\n",
            "Accuracy on evaluation data: 95.96%\n",
            "\n",
            "Lambda is 0.25: regularized\n",
            "Accuracy on evaluation data: 96.05%\n",
            "\n",
            "Lambda is 0.5: regularized\n",
            "Accuracy on evaluation data: 96.21%\n",
            "\n",
            "Lambda is 1: regularized\n",
            "Accuracy on evaluation data: 96.13%\n",
            "\n",
            "Lambda is 2: regularized\n",
            "Accuracy on evaluation data: 95.88%\n",
            "\n",
            "Lambda is 4: regularized\n",
            "Accuracy on evaluation data: 95.67%\n",
            "\n",
            "Lambda is 8: regularized\n",
            "Accuracy on evaluation data: 95.52%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMKskMX57Ez"
      },
      "source": [
        "11. Take a look at the Network.cost_derivative method in network.py. That method was written for the quadratic cost. How would you rewrite the method for the cross-entropy cost? Can you think of a problem that might arise in the cross-entropy version? In network2.py we've eliminated the Nework.cost_derivative method entirely, instead incorporating its functionality into the CrossEntropyCost.delta method.How does this solve the problem you've just identified?\n",
        "  - The current cost-derivative method returns $a^L-y$, the partial derivative of the quadratic cost w.r.t activations. For the cross-entropy version, you can simply replace this return with $-(\\frac{y}{a}-\\frac{1-y}{1-a})$. However, the new return has a chance of being infinite as the denominator approaches $0$ at the nominator = $1$. This is because float64 in Python has the maximum value of $1.79e^{308}$. If $a$ is any number below $(1.79e^{308})^{-1}$ close to $0$, then $\\nabla_aC$ would be infinie. And so backpropagation algorithm won't properly work.\n",
        "  - This problem can be dealth with by cancelling out the denominator and sigmoid prime because $\\sigma'(z) = a(1-a)$. Get rid of the cost derivative method and use $-(\\frac{y}{a}-\\frac{1-y}{1-a}) * \\sigma'(z) = a^L - y$ when you compute delta at the output layer: \\\n",
        "  delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])$\\to$ delta = activations[-1] - y\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wIvtLdnouDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c34aef-cc5b-4424-81b1-cdd414a62cf7"
      },
      "source": [
        "#@title maximum value and minimum value in float64 in Python\n",
        "import sys\n",
        "\n",
        "print(\"maximum:\\t\", sys.float_info.max)\n",
        "print(\"minimum:\\t\", sys.float_info.min)\n",
        "print(\"max^-1:\\t\\t\", sys.float_info.max**-1)\n",
        "print(\"max^-1 < min:\\t\", sys.float_info.max**-1 < sys.float_info.min)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum:\t 1.7976931348623157e+308\n",
            "minimum:\t 2.2250738585072014e-308\n",
            "max^-1:\t\t 5.562684646268003e-309\n",
            "max^-1 < min:\t True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb33vEvLRL5F"
      },
      "source": [
        "12. Modify network2.py so that it implements early stopping using a no-improvement-in-$n$ epochs strategy, where $n$ is a parameter that can be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOKuDHF7jvGo"
      },
      "source": [
        "#@title Network with early stopping (no improvements in n)\n",
        "\"\"\"network2.py\n",
        "~~~~~~~~~~~~~~\n",
        "A modified version of network2.py. Improvements include early stopping, etc.\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Define the quadratic and cross-entropy cost functions\n",
        "class QuadraticCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a, y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output \"y\".\"\"\"\n",
        "          return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.\"\"\"\n",
        "          return (a-y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a,y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output\n",
        "          \"y\".  Note that np.nan_to_num is used to ensure numerical stability. \n",
        "          In particular, if both \"a\" and \"y\" have a 1.0 in the same slot, then \n",
        "          the expression (1-y)*np.log(1-a) returns nan.  The np.nan_to_num \n",
        "          ensures that that is converted to the correct value (0.0).\"\"\"\n",
        "          return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.  Note that the\n",
        "          parameter \"z\" is not used by the method.  It is included in the \n",
        "          method's parameters in order to make the interface consistent with \n",
        "          the delta method for other cost classes.\"\"\"\n",
        "          return (a-y)\n",
        "\n",
        "#### Main Network class\n",
        "class Network_noImp(object):\n",
        "\n",
        "      def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "          \"\"\"The list \"sizes\" contains the number of neurons in the respective\n",
        "          layers of the network.  For example, if the list was [2, 3, 1]\n",
        "          then it would be a three-layer network, with the first layer \n",
        "          containing 2 neurons, the second layer 3 neurons, and the third layer \n",
        "          1 neuron.  The biases and weights for the network are initialized \n",
        "          randomly, using \"self.default_weight_initializer\".\"\"\"\n",
        "          self.num_layers = len(sizes)\n",
        "          self.sizes = sizes\n",
        "          self.default_weight_initializer()\n",
        "          self.cost = cost\n",
        "\n",
        "\n",
        "      def default_weight_initializer(self):\n",
        "          \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1 over the square root of the number of\n",
        "          weights connecting to the same neuron.  Initialize the biases\n",
        "          using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "          Note that the first layer is assumed to be an input layer, and\n",
        "          by convention we won't set any biases for those neurons, since\n",
        "          biases are only ever used in computing the outputs from later layers.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def large_weight_initializer(self):\n",
        "          \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1.  Initialize the biases using a Gaussian \n",
        "          distribution with mean 0 and standard deviation 1. Note that the first\n",
        "          layer is assumed to be an input layer, and by convention we won't set \n",
        "          any biases for those neurons, since biases are only ever used in \n",
        "          computing the outputs from later layers. This weight and bias \n",
        "          initializer uses the same approach as in Chapter 1, and is included \n",
        "          for purposes of comparison.  It will usually be better to use the \n",
        "          default weight initializer instead.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randdn(y, x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def feedforward(self, a):\n",
        "          \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "          for b,w in zip(self.biases, self.weights):\n",
        "              a = sigmoid(np.dot(w,a)+b)\n",
        "          return a\n",
        "\n",
        "\n",
        "      def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
        "              evaluation_data=None, early_stopping=-1, monitor_evaluation_cost=False,\n",
        "              monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
        "              monitor_training_accuracy=False):\n",
        "          \"\"\"Train the neural network using mini-batch stochastic gradient\n",
        "          descent.  The \"training_data\" is a list of tuples \"(x, y)\"\n",
        "          representing the training inputs and the desired outputs.  The\n",
        "          other non-optional parameters are self-explanatory, as is the \n",
        "          regularization parameter \"lmbda\".  The method also accepts \n",
        "          \"evaluation_data\", usually either the validation or test data.  We can\n",
        "          monitor the cost and accuracy on either the evaluation data or the \n",
        "          training data, by setting the appropriate flags. The method returns \n",
        "          a tuple containing four lists: the (per-epoch) costs on the evaluation \n",
        "          data, the accuracies on the evaluation data, the costs on the training\n",
        "          data, and the accuracies on the training data. All values are evaluated \n",
        "          at the end of each training epoch. So, for example, if we train \n",
        "          for 30 epochs, then the first element of the tuple will be a 30-element\n",
        "          list containing the cost on the evaluation data at the end of each epoch. \n",
        "          Note that the lists are empty if the corresponding flag is not set.\"\"\"\n",
        "          if evaluation_data:\n",
        "              evaluation_data = list(evaluation_data)\n",
        "              n_data = len(evaluation_data)\n",
        "          training_data = list(training_data)\n",
        "          n = len(training_data)\n",
        "          evaluation_cost, evaluation_accuracy = [], []\n",
        "          training_cost, training_accuracy = [], []\n",
        "          early_stopping = early_stopping\n",
        "\n",
        "          for j in range(epochs):\n",
        "              random.shuffle(training_data)\n",
        "              mini_batches = [training_data[k:k+mini_batch_size] \n",
        "                              for k in range(0, n, mini_batch_size)]\n",
        "              for mini_batch in mini_batches:\n",
        "                  self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "              print(f\"Epoch {j} training complete\")\n",
        "              if monitor_training_cost:\n",
        "                  cost = self.total_cost(training_data, lmbda)\n",
        "                  training_cost.append(cost)\n",
        "                  print(f\"Cost on training data: {cost}\")\n",
        "              if monitor_training_accuracy:\n",
        "                  accuracy = self.accuracy(training_data, convert=True)\n",
        "                  training_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on training data: {accuracy}/{n}\")\n",
        "              if monitor_evaluation_cost:\n",
        "                  cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                  evaluation_cost.append(cost)\n",
        "                  print(f\"Cost on evaluation data: {cost}\")\n",
        "              if monitor_evaluation_accuracy:\n",
        "                  accuracy = self.accuracy(evaluation_data)\n",
        "                  evaluation_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on evaluation data: {100*accuracy/n_data:.2f}%\")\n",
        "              # Early stopping\n",
        "              i_max = evaluation_accuracy.index(max(evaluation_accuracy))\n",
        "              if len(evaluation_accuracy[i_max:-1]) == early_stopping:\n",
        "                  print(f\"\\nEarly stopped\")\n",
        "                  print(f\"highest classification accuracy: {100*evaluation_accuracy[i_max]/n_data:.2f}% at Epoch {i_max}\")\n",
        "                  break\n",
        "          # return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "      def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "          \"\"\"Update the network's weights and biases by applying gradient\n",
        "          descent using backpropagation to a single mini batch.  The \"mini_batch\" \n",
        "          is a list of tuples \"(x, y)\", \"eta\" is the learning rate, \"lmbda\" is \n",
        "          the regularization parameter, and \"n\" is the total size of the training \n",
        "          data set.\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "          # Partition the given minibatch into two groups: X and Y\n",
        "          X = [mini_batch[i][0] for i in range(len(mini_batch))]\n",
        "          Y = [mini_batch[i][1] for i in range(len(mini_batch))]\n",
        "          delta_nabla_b, delta_nabla_w = self.backprop(X,Y)\n",
        "          nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
        "          nabla_w = [nw+dnw for nw,dnw in zip(nabla_w, delta_nabla_w)]\n",
        "          self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "          self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw \n",
        "                          for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "      def backprop(self, x, y):\n",
        "          \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for \n",
        "          the cost function C_x. \"nabla_b\" and \"nabla_w\" are layer-by-layer lists \n",
        "          of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "          # Make X and Y ndarrays from lists\n",
        "          X = np.concatenate(x, axis=1)\n",
        "          Y = np.concatenate(y, axis=1)\n",
        "          # Feedforward\n",
        "          activation = X\n",
        "          activations = [X] # list to store all the activations, layer by layer \n",
        "          zs = [] # list to store all the z vectors, layer by layer\n",
        "          for b, w in zip(self.biases, self.weights):\n",
        "              z = np.dot(w, activation) + b\n",
        "              zs.append(z)\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "          # Backward pass\n",
        "          delta = (self.cost).delta(zs[-1], activations[-1], Y)\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "          for l in range(2, self.num_layers):\n",
        "              z = zs[-l]\n",
        "              sp = sigmoid_prime(z)\n",
        "              delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "              nabla_b[-l] = delta\n",
        "              nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "          # each ndarry in nabla_b gotta be summed along columns, and reshaped\n",
        "          for i in range(len(nabla_b)):\n",
        "              nabla_b[i] = nabla_b[i].sum(axis=1).reshape(nabla_b[i].shape[0],1)\n",
        "          return nabla_b, nabla_w\n",
        "\n",
        "      def accuracy(self, data, convert=False):\n",
        "          \"\"\"Return the number of inputs in \"data\" for which the neural network \n",
        "          outputs the correct result. The neural network's output is assumed to \n",
        "          be the index of whichever neuron in the final layer has the highest \n",
        "          activation. The flag \"convert\" should be set to False if the data set \n",
        "          is validation or test data (the usual case), and to True if the data \n",
        "          set is the training data. The need for this flag arises due to \n",
        "          differences in the way the results \"y\" are represented in the different \n",
        "          data sets. In particular, it flags whether we need to convert between \n",
        "          the different representations.  It may seem strange to use different\n",
        "          representations for the different data sets.  Why not use the same \n",
        "          representation for all three data sets?  It's done for efficiency \n",
        "          reasons - the program usually evaluates the cost on the training data \n",
        "          and the accuracy on other data sets. These are different types of \n",
        "          computations, and using different representations speeds things up.\n",
        "          More details on the representations can be found in mnist_loader.load_data_wrapper.\"\"\"\n",
        "          if convert:\n",
        "              results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                          for (x,y) in data]\n",
        "          else:\n",
        "              results = [(np.argmax(self.feedforward(x)), y)\n",
        "                          for (x,y) in data]\n",
        "          return sum(int(x==y) for (x,y) in results)\n",
        "\n",
        "      def total_cost(self, data, lmbda, convert=True):\n",
        "          \"\"\"Return the total cost for the data set \"data\". The flag \"convert\" \n",
        "          should be set to False if the data set is the training data (the usual \n",
        "          case), and to True if the data set is the validation or test data.\n",
        "          See comments on the similar (but reversed) convention for the \"accuracy\" method, above.\"\"\"\n",
        "          cost = 0.0\n",
        "          for x, y in data:\n",
        "              a = self.feedforward(x)\n",
        "              if convert:\n",
        "                  y = vectorized_results(y)\n",
        "              cost += self.cost.fn(a,y)/len(data)\n",
        "          cost += 0.5(lmbda/len(data))*sum(np.linalg.norm(w)**2\n",
        "                                           for w in self.weights)\n",
        "          return cost\n",
        "\n",
        "      def save(self, filename):\n",
        "          \"\"\"Save the neural network to the file \"filename\".\"\"\"\n",
        "          data = {\"sizes\": self.sizes,\n",
        "                  \"weights\": [w.tolist() for w in self.weights],\n",
        "                  \"biases\": [b.tolist() for b in self.biases],\n",
        "                  \"cost\": str(self.cost.__name__)}\n",
        "          f = open(filename, \"w\")\n",
        "          json.dump(data, f)        \n",
        "          f.close()\n",
        "\n",
        "\n",
        "#### Loading a Network\n",
        "def load(filename):\n",
        "    \"\"\"Load a neural network from the file \"filename\". Returns an instance of Network.\"\"\"\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position and \n",
        "    zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding \n",
        "    desired output from the neural network.\"\"\"\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k5e2Uq5kSbH"
      },
      "source": [
        "#@title Download the MNIST data set and minist_loader.py\n",
        "#Download the dataset from Nielsen's github\n",
        "!wget -L https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
        "\n",
        "#Download mnist_loader.py from Dobrzanski's github: his version's for python 3\n",
        "!wget -L https://raw.githubusercontent.com/MichalDanielDobrzanski/DeepLearningPython/master/mnist_loader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6suVGNuqkUde"
      },
      "source": [
        "#@title Early stopping using a no-improvement  in 10\n",
        "import mnist_loader\n",
        "\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "net_noImp = Network_noImp([784, 30, 10])  \n",
        "net_noImp.SGD(training_data, 50, 10, 0.5, lmbda=0.5, evaluation_data=validation_data, early_stopping=10, monitor_evaluation_accuracy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XniVeVOuO7Sx"
      },
      "source": [
        "13. Can you think of a rule for early stopping other than no-improvement-in-$n$? Ideally, the rule should compromise between getting high validation accuracies and not training too long. Add your rule to network2.py and run three experiments comparing the validation accuracies and number of epochs of training to no-improvement-in-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEcNSUUkg_l"
      },
      "source": [
        "14. Add momentum-based stochastic descent to network2.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3BiJqoFNXw"
      },
      "source": [
        "#@title Network with momentum-based SGD\n",
        "\"\"\"network2.py\n",
        "~~~~~~~~~~~~~~\n",
        "A modified version of network2.py. Improvements include momentum-based SGD.\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Define the quadratic and cross-entropy cost functions\n",
        "class QuadraticCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a, y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output \"y\".\"\"\"\n",
        "          return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.\"\"\"\n",
        "          return (a-y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "      @staticmethod\n",
        "      def fn(a,y):\n",
        "          \"\"\"Return the cost associated with an output \"a\" and desired output\n",
        "          \"y\".  Note that np.nan_to_num is used to ensure numerical stability. \n",
        "          In particular, if both \"a\" and \"y\" have a 1.0 in the same slot, then \n",
        "          the expression (1-y)*np.log(1-a) returns nan.  The np.nan_to_num \n",
        "          ensures that that is converted to the correct value (0.0).\"\"\"\n",
        "          return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "      @staticmethod\n",
        "      def delta(z, a, y):\n",
        "          \"\"\"Return the error delta from the output layer.  Note that the\n",
        "          parameter \"z\" is not used by the method.  It is included in the \n",
        "          method's parameters in order to make the interface consistent with \n",
        "          the delta method for other cost classes.\"\"\"\n",
        "          return (a-y)\n",
        "\n",
        "#### Main Network class\n",
        "class Network_momentum(object):\n",
        "\n",
        "      def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "          \"\"\"The list \"sizes\" contains the number of neurons in the respective\n",
        "          layers of the network.  For example, if the list was [2, 3, 1]\n",
        "          then it would be a three-layer network, with the first layer \n",
        "          containing 2 neurons, the second layer 3 neurons, and the third layer \n",
        "          1 neuron.  The biases and weights for the network are initialized \n",
        "          randomly, using \"self.default_weight_initializer\".\"\"\"\n",
        "          self.num_layers = len(sizes)\n",
        "          self.sizes = sizes\n",
        "          self.default_weight_initializer()\n",
        "          self.cost = cost\n",
        "\n",
        "\n",
        "      def default_weight_initializer(self):\n",
        "          \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1 over the square root of the number of\n",
        "          weights connecting to the same neuron.  Initialize the biases\n",
        "          using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "          Note that the first layer is assumed to be an input layer, and\n",
        "          by convention we won't set any biases for those neurons, since\n",
        "          biases are only ever used in computing the outputs from later layers.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "          self.velocity = [np.zeros(shape=(y, x)) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def large_weight_initializer(self):\n",
        "          \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "          and standard deviation 1.  Initialize the biases using a Gaussian \n",
        "          distribution with mean 0 and standard deviation 1. Note that the first\n",
        "          layer is assumed to be an input layer, and by convention we won't set \n",
        "          any biases for those neurons, since biases are only ever used in \n",
        "          computing the outputs from later layers. This weight and bias \n",
        "          initializer uses the same approach as in Chapter 1, and is included \n",
        "          for purposes of comparison.  It will usually be better to use the \n",
        "          default weight initializer instead.\"\"\"\n",
        "          self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "          self.weights = [np.random.randdn(y, x) \n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "          self.velocity = [np.zeros(shape=(y, x))\n",
        "                          for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
        "\n",
        "\n",
        "      def feedforward(self, a):\n",
        "          \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
        "          for b,w in zip(self.biases, self.weights):\n",
        "              a = sigmoid(np.dot(w,a)+b)\n",
        "          return a\n",
        "\n",
        "\n",
        "      def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
        "              mu = 0.0, evaluation_data=None, monitor_evaluation_cost=False,\n",
        "              monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
        "              monitor_training_accuracy=False):\n",
        "          \"\"\"Train the neural network using mini-batch stochastic gradient\n",
        "          descent.  The \"training_data\" is a list of tuples \"(x, y)\"\n",
        "          representing the training inputs and the desired outputs.  The\n",
        "          other non-optional parameters are self-explanatory, as is the \n",
        "          regularization parameter \"lmbda\".  The method also accepts \n",
        "          \"evaluation_data\", usually either the validation or test data.  We can\n",
        "          monitor the cost and accuracy on either the evaluation data or the \n",
        "          training data, by setting the appropriate flags. The method returns \n",
        "          a tuple containing four lists: the (per-epoch) costs on the evaluation \n",
        "          data, the accuracies on the evaluation data, the costs on the training\n",
        "          data, and the accuracies on the training data. All values are evaluated \n",
        "          at the end of each training epoch. So, for example, if we train \n",
        "          for 30 epochs, then the first element of the tuple will be a 30-element\n",
        "          list containing the cost on the evaluation data at the end of each epoch. \n",
        "          Note that the lists are empty if the corresponding flag is not set.\"\"\"\n",
        "          if evaluation_data:\n",
        "              evaluation_data = list(evaluation_data)\n",
        "              n_data = len(evaluation_data)\n",
        "          training_data = list(training_data)\n",
        "          n = len(training_data)\n",
        "          evaluation_cost, evaluation_accuracy = [], []\n",
        "          training_cost, training_accuracy = [], []\n",
        "\n",
        "          for j in range(epochs):\n",
        "              random.shuffle(training_data)\n",
        "              mini_batches = [training_data[k:k+mini_batch_size] \n",
        "                              for k in range(0, n, mini_batch_size)]\n",
        "              for mini_batch in mini_batches:\n",
        "                  self.update_mini_batch(mini_batch, eta, lmbda, mu, len(training_data))\n",
        "              print(f\"Epoch {j} training complete\")\n",
        "              if monitor_training_cost:\n",
        "                  cost = self.total_cost(training_data, lmbda)\n",
        "                  training_cost.append(cost)\n",
        "                  print(f\"Cost on training data: {cost}\")\n",
        "              if monitor_training_accuracy:\n",
        "                  accuracy = self.accuracy(training_data, convert=True)\n",
        "                  training_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on training data: {accuracy}/{n}\")\n",
        "              if monitor_evaluation_cost:\n",
        "                  cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                  evaluation_cost.append(cost)\n",
        "                  print(f\"Cost on evaluation data: {cost}\")\n",
        "              if monitor_evaluation_accuracy:\n",
        "                  accuracy = self.accuracy(evaluation_data)\n",
        "                  evaluation_accuracy.append(accuracy)\n",
        "                  print(f\"Accuracy on evaluation data: {100*accuracy/n_data:.2f}%\")\n",
        "#          return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "      def update_mini_batch(self, mini_batch, eta, lmbda, mu, n):\n",
        "          \"\"\"Update the network's weights and biases by applying gradient\n",
        "          descent using backpropagation to a single mini batch.  The \"mini_batch\" \n",
        "          is a list of tuples \"(x, y)\", \"eta\" is the learning rate, \"lmbda\" is \n",
        "          the regularization parameter, and \"n\" is the total size of the training \n",
        "          data set.\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "          # Partition the given minibatch into two groups: X and Y\n",
        "          X = [mini_batch[i][0] for i in range(len(mini_batch))]\n",
        "          Y = [mini_batch[i][1] for i in range(len(mini_batch))]\n",
        "          delta_nabla_b, delta_nabla_w = self.backprop(X,Y)\n",
        "          nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
        "          nabla_w = [nw+dnw for nw,dnw in zip(nabla_w, delta_nabla_w)]\n",
        "          self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "          self.velocity = [mu*v - (eta/len(mini_batch))*nw for v, nw in zip(self.velocity, nabla_w)]\n",
        "          self.weights = [(1-eta*(lmbda/n))*w + v for w,v in zip(self.weights, self.velocity)]\n",
        "\n",
        "      def backprop(self, x, y):\n",
        "          \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for \n",
        "          the cost function C_x. \"nabla_b\" and \"nabla_w\" are layer-by-layer lists \n",
        "          of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "          nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "          nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "          # Make X and Y ndarrays from lists\n",
        "          X = np.concatenate(x, axis=1)\n",
        "          Y = np.concatenate(y, axis=1)\n",
        "          # Feedforward\n",
        "          activation = X\n",
        "          activations = [X] # list to store all the activations, layer by layer \n",
        "          zs = [] # list to store all the z vectors, layer by layer\n",
        "          for b, w in zip(self.biases, self.weights):\n",
        "              z = np.dot(w, activation) + b\n",
        "              zs.append(z)\n",
        "              activation = sigmoid(z)\n",
        "              activations.append(activation)\n",
        "          # Backward pass\n",
        "          delta = (self.cost).delta(zs[-1], activations[-1], Y)\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "          for l in range(2, self.num_layers):\n",
        "              z = zs[-l]\n",
        "              sp = sigmoid_prime(z)\n",
        "              delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "              nabla_b[-l] = delta\n",
        "              nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "          # each ndarry in nabla_b gotta be summed along columns, and reshaped\n",
        "          for i in range(len(nabla_b)):\n",
        "              nabla_b[i] = nabla_b[i].sum(axis=1).reshape(nabla_b[i].shape[0],1)\n",
        "          return nabla_b, nabla_w\n",
        "\n",
        "      def accuracy(self, data, convert=False):\n",
        "          \"\"\"Return the number of inputs in \"data\" for which the neural network \n",
        "          outputs the correct result. The neural network's output is assumed to \n",
        "          be the index of whichever neuron in the final layer has the highest \n",
        "          activation. The flag \"convert\" should be set to False if the data set \n",
        "          is validation or test data (the usual case), and to True if the data \n",
        "          set is the training data. The need for this flag arises due to \n",
        "          differences in the way the results \"y\" are represented in the different \n",
        "          data sets. In particular, it flags whether we need to convert between \n",
        "          the different representations.  It may seem strange to use different\n",
        "          representations for the different data sets.  Why not use the same \n",
        "          representation for all three data sets?  It's done for efficiency \n",
        "          reasons - the program usually evaluates the cost on the training data \n",
        "          and the accuracy on other data sets. These are different types of \n",
        "          computations, and using different representations speeds things up.\n",
        "          More details on the representations can be found in mnist_loader.load_data_wrapper.\"\"\"\n",
        "          if convert:\n",
        "              results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                          for (x,y) in data]\n",
        "          else:\n",
        "              results = [(np.argmax(self.feedforward(x)), y)\n",
        "                          for (x,y) in data]\n",
        "          return sum(int(x==y) for (x,y) in results)\n",
        "\n",
        "      def total_cost(self, data, lmbda, convert=True):\n",
        "          \"\"\"Return the total cost for the data set \"data\". The flag \"convert\" \n",
        "          should be set to False if the data set is the training data (the usual \n",
        "          case), and to True if the data set is the validation or test data.\n",
        "          See comments on the similar (but reversed) convention for the \"accuracy\" method, above.\"\"\"\n",
        "          cost = 0.0\n",
        "          for x, y in data:\n",
        "              a = self.feedforward(x)\n",
        "              if convert:\n",
        "                  y = vectorized_results(y)\n",
        "              cost += self.cost.fn(a,y)/len(data)\n",
        "          cost += 0.5(lmbda/len(data))*sum(np.linalg.norm(w)**2\n",
        "                                           for w in self.weights)\n",
        "          return cost\n",
        "\n",
        "      def save(self, filename):\n",
        "          \"\"\"Save the neural network to the file \"filename\".\"\"\"\n",
        "          data = {\"sizes\": self.sizes,\n",
        "                  \"weights\": [w.tolist() for w in self.weights],\n",
        "                  \"biases\": [b.tolist() for b in self.biases],\n",
        "                  \"cost\": str(self.cost.__name__)}\n",
        "          f = open(filename, \"w\")\n",
        "          json.dump(data, f)        \n",
        "          f.close()\n",
        "\n",
        "\n",
        "#### Loading a Network\n",
        "def load(filename):\n",
        "    \"\"\"Load a neural network from the file \"filename\". Returns an instance of Network.\"\"\"\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position and \n",
        "    zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding \n",
        "    desired output from the neural network.\"\"\"\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yY77QTs2TjO"
      },
      "source": [
        "#@title Download the MNIST data set and minist_loader.py\n",
        "#Download the dataset from Nielsen's github\n",
        "!wget -L https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
        "\n",
        "#Download mnist_loader.py from Dobrzanski's github: his version's for python 3\n",
        "!wget -L https://raw.githubusercontent.com/MichalDanielDobrzanski/DeepLearningPython/master/mnist_loader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L8V4m7k2T6m"
      },
      "source": [
        "#@title Momentum-based SGD\n",
        "import mnist_loader\n",
        "\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "net_momentum = Network_momentum([784, 30, 10])  \n",
        "net_momentum.SGD(training_data, 30, 10, 0.5, lmbda=0.5, mu=0.2, evaluation_data=validation_data, monitor_evaluation_accuracy=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}